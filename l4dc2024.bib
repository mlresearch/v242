@Proceedings{l4dc2024,
    booktitle = {Proceedings of the 6th Annual Learning for Dynamics \& Control Conference},
    title = {Proceedings of the 6th Annual Learning for Dynamics \& Control Conference},
    name = {6th Annual Learning for Dynamics \& Control Conference},
    shortname = {L4DC},
    year = {2024},
    editor = {Abate, Alessandro and Margellos, Kostas and Papachristodoulou, Antonis},
    volume = {242},
    start = {2024-07-15},
    end = {2024-07-17},
    address = {University of Oxford, Oxford, UK},
    conference_url = {https://l4dc.web.ox.ac.uk/},
}

@InProceedings{chen3,
    title = {Leveraging {H}amilton-{J}acobi {PDE}s with time-dependent Hamiltonians for continual scientific machine learning},
    author = {Chen, Paula and Meng, Tingwei and Zou, Zongren and Darbon, J\'{e}r\^{o}me and Karniadakis, George Em},
    pages = {1--12},
    abstract = {We address two major challenges in scientific machine learning (SciML): interpretability and computational efficiency. We increase the interpretability of certain learning processes by establishing a new theoretical connection between optimization problems arising from SciML and a generalized Hopf formula, which represents the viscosity solution to a Hamilton-Jacobi partial differential equation (HJ PDE) with time-dependent Hamiltonian. Namely, we show that when we solve certain regularized learning problems with integral-type losses, we actually solve an optimal control problem and its associated HJ PDE with time-dependent Hamiltonian. This connection allows us to reinterpret incremental updates to learned models as the evolution of an associated HJ PDE and optimal control problem in time, where all of the previous information is intrinsically encoded in the solution to the HJ PDE. As a result, existing HJ PDE solvers and optimal control algorithms can be reused to design new efficient training approaches for SciML that naturally coincide with the continual learning framework, while avoiding catastrophic forgetting. As a first exploration of this connection, we consider the special case of linear regression and leverage our connection to develop a new Riccati-based methodology for solving these learning problems that is amenable to continual learning applications. We also provide some corresponding numerical examples that demonstrate the potential computational and memory advantages our Riccati-based approach can provide.}
}

@InProceedings{salehi8,
    title = {Data-efficient, Explainable and Safe Box Manipulation: Illustrating the Advantages of Physical Priors in Model-Predictive Control},
    author = {Salehi, Achkan and Doncieux, Stephane},
    pages = {13--24},
    abstract = {Model-based RL/control have gained significant traction in robotics. Yet, these approaches often remain data-inefficient and lack the explainability of hand-engineered solutions. This makes them difficult to debug/integrate in safety-critical settings. However, in many systems, prior knowledge of environment kinematics/dynamics is available. Incorporating such priors can help address the aforementioned problems by reducing problem complexity and the need for exploration, while also facilitating the expression of the decisions taken by the agent in terms of physically meaningful entities. Our aim with this paper is to illustrate and support this point of view via a case-study. We model a payload manipulation problem based on a real robotic system, and show that leveraging prior knowledge about the dynamics of the environment in an MPC framework can lead to improvements in explainability, safety and data-efficiency, leading to satisfying generalization properties with less data.}
}

@InProceedings{yao11,
    title = {Gradient Shaping for Multi-Constraint Safe Reinforcement Learning},
    author = {Yao, Yihang and Liu, Zuxin and Cen, Zhepeng and Huang, Peide and Zhang, Tingnan and Yu, Wenhao and Zhao, Ding},
    pages = {25--39},
    abstract = {Online safe reinforcement learning (RL) involves training a policy that maximizes task efficiency while satisfying constraints via interacting with the environments. In this paper, our focus lies in addressing the complex challenges associated with solving multi-constraint (MC) safe RL problems. We approach the Safe RL problem from the perspective of Multi-Objective Optimization (MOO) and propose a unified framework designed for MC safe RL algorithms. This framework highlights the manipulation of gradients derived from constraints. Leveraging insights from this framework and recognizing the significance of redundant and conflicting constraint conditions, we introduce the Gradient Shaping (GradS) method for general Lagrangian-based safe RL algorithms to improve the training efficiency in terms of both reward and constraint satisfaction. Our extensive experimentation demonstrates the effectiveness of our proposed method in encouraging exploration and learning a policy that improves both safety and reward performance across various challenging MC safe RL tasks as well as good scalability to the constraint dimension.}
}

@InProceedings{akgul12,
    title = {Continual Learning of Multi-modal Dynamics with External Memory},
    author = {Akg\"{u}l, Abdullah and Unal, Gozde and Kandemir, Melih},
    pages = {40--51},
    abstract = {We study the problem of fitting a model to a dynamical environment when new modes of behavior emerge sequentially. The learning model is aware when a new mode appears, but it cannot access the true modes of individual training sequences. The state-of-the-art continual learning approaches cannot handle this setup, because parameter transfer suffers from catastrophic interference and episodic memory design requires the knowledge of the ground-truth modes of sequences. We devise a novel continual learning method that overcomes both limitations by maintaining a descriptor of the mode of an encountered sequence in a neural episodic memory. We employ a Dirichlet Process prior on the attention weights of the memory to foster efficient storage of the mode descriptors. Our method performs continual learning by transferring knowledge across tasks by retrieving the descriptors of similar modes of past tasks to the mode of a current sequence and feeding this descriptor into its transition kernel as control input. We observe the continual learning performance of our method to compare favorably to the mainstream parameter transfer approach.}
}

@InProceedings{zhang13,
    title = {Learning to Stabilize High-dimensional Unknown Systems Using Lyapunov-guided Exploration},
    author = {Zhang, Songyuan and Fan, Chuchu},
    pages = {52--67},
    abstract = {Designing stabilizing controllers is a fundamental challenge in autonomous systems, particularly for high-dimensional, nonlinear systems that can hardly be accurately modeled with differential equations. The Lyapunov theory offers a solution for stabilizing control systems, still, current methods relying on Lyapunov functions require access to complete dynamics or samples of system executions throughout the entire state space. Consequently, they are impractical for high-dimensional systems. This paper introduces a novel framework, LYapunov-Guided Exploration (LYGE), for learning stabilizing controllers tailored to high-dimensional, unknown systems. LYGE employs Lyapunov theory to iteratively guide the search for samples during exploration while simultaneously learning the local system dynamics, control policy, and Lyapunov functions. We demonstrate its scalability on highly complex systems, including a high-fidelity F-16 jet model featuring a 16D state space and a 4D input space. Experiments indicate that, compared to prior works in reinforcement learning, imitation learning, and neural certificates, LYGE reduces the distance to the goal by 50\% whil<e requiring only 5\% to 32\% of the samples. Furthermore, we demonstrate that our algorithm can be extended to learn controllers guided by other certificate functions for unknown systems.}
}

@InProceedings{barkley14,
    title = {An Investigation of Time Reversal Symmetry in Reinforcement Learning},
    author = {Barkley, Brett and Zhang, Amy and Fridovich-Keil, David},
    pages = {68--79},
    abstract = {One of the fundamental challenges associated with reinforcement learning (RL) is that collecting sufficient data can be both time-consuming and expensive. In this paper, we formalize a concept of time reversal symmetry in a Markov decision process (MDP), which builds upon the established structure of dynamically reversible Markov chains (DRMCs) and time-reversibility in classical physics. Specifically, we investigate the utility of this concept in reducing the sample complexity of reinforcement learning. We observe that utilizing the structure of time reversal in an MDP allows every environment transition experienced by an agent to be transformed into a feasible reverse-time transition, effectively doubling the number of experiences in the environment. To test the usefulness of this newly synthesized data, we develop a novel approach called time symmetric data augmentation (TSDA) and investigate its application in both proprioceptive and pixel-based state within the realm of off-policy, model-free RL. Empirical evaluations showcase how these synthetic transitions can enhance the sample efficiency of RL agents in time reversible scenarios without friction or contact. We also test this method in more realistic environments where these assumptions are not globally satisfied. We find that TSDA can significantly degrade sample efficiency and policy performance, but can also improve sample efficiency under the right conditions. Ultimately we conclude that time symmetry shows promise in enhancing the sample efficiency of reinforcement learning and provide guidance when the environment and reward structures are of an appropriate form for TSDA to be employed effectively.}
}

@InProceedings{yan16,
    title = {{HSVI}-based Online Minimax Strategies for Partially Observable Stochastic Games with Neural Perception Mechanisms},
    author = {Yan, Rui and Santos, Gabriel and Norman, Gethin and Parker, David and Kwiatkowska, Marta},
    pages = {80--91},
    abstract = {We consider a variant of continuous-state partially-observable stochastic games with neural perception mechanisms and an asymmetric information structure. One agent has partial information, with the observation function implemented as a neural network, while the other agent is assumed to have full knowledge of the state. We present, for the first time, an efficient online method to compute an $\varepsilon$-minimax strategy profile, which requires only one linear program to be solved for each agent at every stage, instead of a complex estimation of opponent counterfactual values. For the partially-informed agent, we propose a continual resolving approach which uses lower bounds, pre-computed offline with heuristic search value iteration (HSVI), instead of opponent counterfactual values. This inherits the soundness of continual resolving at the cost of pre-computing the bound. For the fully-informed agent, we propose an inferred-belief strategy, where the agent maintains an inferred belief about the belief of the partially-informed agent based on (offline) upper bounds from HSVI, guaranteeing $\varepsilon$-distance to the value of the game at the initial belief known to both agents.}
}

@InProceedings{hu17,
    title = {Real-Time Safe Control of Neural Network Dynamic Models with Sound Approximation},
    author = {Hu, Hanjiang and Lan, Jianglin and Liu, Changliu},
    pages = {92--103},
    abstract = {Safe control of neural network dynamic models (NNDMs) is important to robotics and many applications. However, it remains challenging to compute an optimal safe control in real time for NNDM. To enable real-time computation, we propose to use a sound approximation of the NNDM in the control synthesis. In particular, we propose Bernstein over-approximated neural dynamics (BOND) based on the Bernstein polynomial over-approximation (BPO) of ReLU activation functions in NNDM. To mitigate the errors introduced by the approximation and to ensure persistent feasibility of the safe control problems, we synthesize a worst-case safety index using the most unsafe approximated state within the BPO relaxation of NNDM offline. For the online real-time optimization, we formulate the first-order Taylor approximation of the nonlinear worst-case safety constraint as an additional linear layer of NNDM with the l2 bounded bias term for the higher-order remainder. Comprehensive experiments with different neural dynamics and safety constraints show that with safety guaranteed, our NNDMs with sound approximation are 10-100 times faster than the safe control baseline that uses mixed integer programming (MIP), validating the effectiveness of the worst-case safety index and scalability of the proposed BOND in real-time large-scale settings.}
}

@InProceedings{cramer19,
    title = {Tracking Object Positions in Reinforcement Learning: A Metric for Keypoint Detection},
    author = {Cramer, Emma and Reiher, Jonas and Trimpe, Sebastian},
    pages = {104--116},
    abstract = {Reinforcement learning (RL) for robot control typically requires a detailed representation of the environment state, including information about task-relevant objects not directly measurable. Keypoint detectors, such as spatial autoencoders (SAEs), are a common approach to extracting a low-dimensional representation from high-dimensional image data. SAEs aim at spatial features such as object positions, which are often useful representations in robotic RL. However, whether an SAE is actually able to track objects in the scene and thus yields a spatial state representation well suited for RL tasks has rarely been examined due to a lack of established metrics. In this paper, we propose to assess the performance of an SAE instance by measuring how well keypoints track ground truth objects in images. We present a computationally lightweight metric and use it to evaluate common baseline SAE architectures on image data from a simulated robot task. We find that common SAEs differ substantially in their spatial extraction capability. Furthermore, we validate that SAEs that perform well in our metric achieve superior performance when used in downstream RL. Thus, our metric is an effective and lightweight indicator of RL performance before executing expensive RL training. Building on these insights, we identify three key modifications of SAE architectures to improve tracking performance. We make our code available at https://anonymous.4open.science/r/sae-rl.}
}

@InProceedings{hinderyckx20,
    title = {Linearised Data-Driven {LSTM}-Based Control of Multi-Input {HVAC} Systems},
    author = {Hinderyckx, Andreas and Guillaume, Florence},
    pages = {117--129},
    abstract = {The pursuit of sustainability has paved the way for building management systems (BMSs) that can steer buildings in an energy-efficient way. In this article, a deep learning approach is proposed to control multi-input HVAC systems in order to minimize both thermal discomfort and operational cost. More particularly, an LSTM-based encoder-decoder process model, trained on historical weather data and control sequences generated while the building was steered by a modern rule-based controller (RBC), is fed into an optimisation problem, to which a change of variables is applied to efficiently model the effect of interdependent control inputs. Both the nonlinear LSTM process model and the cost function of the optimisation problem are linearised to formulate the control problem as a mixed integer linear programming (MILP) problem, which ensures that the controller can operate in near real-time and with limited computational power. Moreover, to avoid resorting to model extrapolation and to improve the model's predictive performance, the set of allowed control signal values is restricted using a quantile-based approach. In addition to the purely data-driven controller (DDC), a hybrid controller is designed to leverage the strengths of the RBC and the DDC. The performance of both controllers is benchmarked against the RBC's performance using the BOPTEST simulation environment under various experiment settings, highlighting how the hyperparameters affect the controller's performance. Compared to the RBC, we show that the proposed controllers realise substantial improvements in terms of both thermal comfort and operational cost while controlling a single zone or two zones simultaneously.}
}

@InProceedings{markovsky22,
    title = {The {B}ehavioral {T}oolbox},
    author = {Markovsky, Ivan},
    pages = {130--141},
    abstract = {The Behavioral Toolbox is a collection of Matlab functions for modeling, analysis, and design of dynamical systems using the behavioral approach to systems theory and control. It implements newly emerged direct data-driven methods as well as classical parametric representations of linear time-invariant systems. At the core of the toolbox is a nonparameteric representation of the finite-horizon behavior by an orthonormal basis. The current version has education and research goals and isn't intended for handling ``big data''. The paper presents five problems --- checking systems equality, interconnection of systems, errors-in-variables least-squares smoothing, missing input estimation, and data-driven forecasting --- and describes their solution by the methods in the toolbox.}
}

@InProceedings{zhao24,
    title = {Learning ``Look-Ahead'' Nonlocal Traffic Dynamics in a Ring Road},
    author = {Zhao, Chenguang and Yu, Huan},
    pages = {142--154},
    abstract = {The macroscopic traffic flow model is widely used for traffic control and management. To incorporate drivers' anticipative behaviors and to remove impractical speed discontinuity inherent in the classic Lighthill–Whitham–Richards (LWR) traffic model, nonlocal partial differential equation (PDE) models with ``look-ahead'' dynamics have been proposed, which assume that the speed is a function of weighted downstream traffic density. However, it lacks data validation on two important questions: whether there exist nonlocal dynamics, and how the length and weight of the ``look-ahead'' window affect the spatial temporal propagation of traffic densities. In this paper, we adopt traffic trajectory data from a ring-road experiment and design a physics-informed neural network to learn the fundamental diagram and look-ahead kernel that best fit the data, and reinvent a data-enhanced nonlocal LWR model via minimizing the loss function combining the data discrepancy and the nonlocal model discrepancy. Results show that the learned nonlocal LWR yields a more accurate prediction of traffic wave propagation in three different scenarios: stop-and-go oscillations, congested, and free traffic. We first demonstrate the existence of ``look-ahead'' effect with real traffic data. The optimal nonlocal kernel is found out to take a length of around 35 to 50 meters, and the kernel weight within 5 meters accounts for the majority of the nonlocal effect. Our results also underscore the importance of choosing a priori physics in machine learning models.}
}

@InProceedings{turan25,
    title = {Safe Dynamic Pricing for Nonstationary Network Resource Allocation},
    author = {Turan, Berkay and Hutchinson, Spencer and Alizadeh, Mahnoosh},
    pages = {155--167},
    abstract = {This paper introduces the Safe Pricing for NUM with Gradual Variations (SPNUM-GV) algorithm, addressing challenges in pricing-based distributed resource allocation for safety-critical systems with non-stationary utility functions. Focusing on domains where 1) users' optimal demand can only be induced through posted prices, 2) real-time two-way communication with the users is not available, 3) the induced demand must always belong to an arbitrarily shaped convex and compact feasible set in spite of price response uncertainty, and 4) the users' response to prices are evolving over time, we design SPNUM-GV to generate prices that ensure stage-wise safety of the induced demand while achieving sublinear regret. SPNUM-GV ensures safety by determining a ``desired demand'' within a shrunk feasible set using a projected gradient method and updating the prices to induce a demand close to the desired demand by leveraging an estimate of the users' price response function. By tuning the amount of shrinkage to account for the error between the desired and the induced demand, we prove that the induced demand always belongs to the feasible set. In addition, we prove that the regret incurred by the induced demand is ${\cal O}(\sqrt{T(1+V_T)})$ after $T$ iterations, where $V_T$ is an upper bound on the total gradual variations of the users' utility functions. Numerical simulations demonstrate the efficacy of SPNUM-GV and support our theoretical findings.}
}

@InProceedings{hutchinson27,
    title = {Safe Online Convex Optimization with Multi-Point Feedback},
    author = {Hutchinson, Spencer and Alizadeh, Mahnoosh},
    pages = {168--180},
    abstract = {Motivated by the stringent safety requirements that are often present in real-world applications, we study a safe online convex optimization setting where the player needs to simultaneously achieve sublinear regret and zero constraint violation while only using zero-order information. In particular, we consider a multi-point feedback setting, where the player chooses $d + 1$ points in each round (where $d$ is the problem dimension) and then receives the value of the constraint function and cost function at each of these points. To address this problem, we propose an algorithm that leverages forward-difference gradient estimation as well as optimistic and pessimistic action sets to achieve $O(d \sqrt{T})$ regret and zero constraint violation under the assumption that the constraint function is smooth and strongly convex. We then perform a numerical study to investigate the impacts of the unknown constraint and zero-order feedback on empirical performance.}
}

@InProceedings{zhang28,
    title = {Controlgym: Large-Scale Control Environments for Benchmarking Reinforcement Learning Algorithms},
    author = {Zhang, Xiangyuan and Mao, Weichao and Mowlavi, Saviz and Benosman, Mouhacine and Basar, Tamer},
    pages = {181--196},
    abstract = {We introduce controlgym, a library of thirty-six industrial control settings, and ten infinite-dimensional partial differential equation (PDE)-based control problems. Integrated within the OpenAI Gym/Gymnasium (Gym) framework, controlgym allows direct applications of standard reinforcement learning (RL) algorithms like stable-baselines3. Our control environments complement those in Gym with continuous, unbounded action and observation spaces, motivated by real-world control applications. Moreover, the PDE control environments uniquely allow the users to extend the state dimensionality of the system to infinity while preserving the intrinsic dynamics. This feature is crucial for evaluating the scalability of RL algorithms for control. This project serves the learning for dynamics \& control (L4DC) community, aiming to explore key questions: the convergence of RL algorithms in learning control policies; the stability and robustness issues of learning-based controllers; and the scalability of RL algorithms to high- and potentially infinite-dimensional systems. We open-source the controlgym project at https://github.com/xiangyuan-zhang/controlgym.}
}

@InProceedings{latafat31,
    title = {On the convergence of adaptive first order methods: proximal gradient and alternating minimization algorithms},
    author = {Latafat, Puya and Themelis, Andreas and Patrinos, Panagiotis},
    pages = {197--208},
    abstract = {Building upon recent works on linesearch-free adaptive proximal gradient methods, this paper proposes AdaPG, a framework that unifies and extends existing results by providing larger stepsize policies and improved lower bounds. Different choices of the parameters are discussed and the efficacy of the resulting methods is demonstrated through numerical simulations. In an attempt to better understand the underlying theory, its convergence is established in a more general setting that allows for time-varying parameters. Finally, an adaptive alternating minimization algorithm is presented by exploring the dual setting. This algorithm not only incorporates additional adaptivity but also expands its applicability beyond standard strongly convex settings.}
}

@InProceedings{richardson33,
    title = {Strengthened stability analysis of discrete-time Lurie systems involving {ReLU} neural networks},
    author = {Richardson, Carl and Turner, Matthew and Gunn, Steve and Drummond, Ross},
    pages = {209--221},
    abstract = {This paper addresses the stability analysis of a discrete-time (DT) Lurie system featuring a static repeated ReLU nonlinearity. Such systems often arise in the analysis of recurrent neural networks and other neural feedback loops. Custom quadratic constraints, satisfied by the repeated ReLU, are employed to strengthen the standard Circle and Popov Criteria for this specific Lurie system. The criteria can be expressed as a set of linear matrix inequalities (LMIs) with less restrictive conditions on the matrix variables. It is further shown that if the Lurie system under consideration has a unique equilibrium point at the origin, then this equilibrium point is in fact globally stable or unstable, meaning that local stability analysis will provide no additional benefit. Numerical examples demonstrate that the strengthened criteria achieve a desirable balance between reduced conservatism and complexity when compared to existing criteria.}
}

@InProceedings{henkel34,
    title = {Interpretable Data-Driven Model Predictive Control of Building Energy Systems using {SHAP}},
    author = {Henkel, Patrick and Kasperski, Tobias and Stoffel, Phillip and M\"{u}ller, Dirk},
    pages = {222--234},
    abstract = {Advanced building energy system controls, such as model predictive control, rely on accurate system models. To reduce the modelling effort in the building sector, data-driven models are becoming increasingly popular in research. Despite their promising performance, data-driven models are considered black boxes. This black box nature is an obstacle to widespread application, as it is difficult for building operators to understand how predictions are made. Concepts known as Explainable Artificial Intelligence are being developed to improve the interpretability of black box models. This work combines the popular Explainable Artificial Intelligence method Shapley Additive Explanations (SHAP) with data-driven model predictive control to increase the interpretability of artificial neural networks used as process models during model creation. Using a standardised residual building energy system for controller testing, an in-depth analysis of how the models make predictions is carried out. In addition, the influence of different model setups on the control performance is evaluated. The results show that the different control performances can be justified by analysing the underlying models with SHAP. SHAP shows how the characteristics of a feature affect the prediction and reveals weaknesses in the model. In addition, the features can be sorted according to their influence on the prediction, which is utilized for feature selection.}
}

@InProceedings{pilar35,
    title = {Physics-informed Neural Networks with Unknown Measurement Noise},
    author = {Pilar, Philipp and Wahlstr\"{o}m, Niklas},
    pages = {235--247},
    abstract = {Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated with weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples.}
}

@InProceedings{mhaisen36,
    title = {Adaptive Online Non-stochastic Control},
    author = {Mhaisen, Naram and Iosifidis, George},
    pages = {248--259},
    abstract = {We tackle the problem of Non-stochastic Control (NSC) with the aim of obtaining algorithms whose policy regret is proportional to the difficulty of the controlled environment. Namely, we tailor the Follow The Regularized Leader (FTRL) framework to dynamical systems by using regularizers that are proportional to the actual witnessed costs. The main challenge arises from using the proposed adaptive regularizers in the presence of a state, or equivalently, a memory, which couples the effect of the online decisions and requires new tools for bounding the regret. Via new analysis techniques for NSC and FTRL integration, we obtain novel disturbance action controllers (DAC) with sub-linear data adaptive policy regret bounds that shrink when the trajectory of costs has small gradients, while staying sub-linear even in the worst case.}
}

@InProceedings{hoppe38,
    title = {Global Rewards in Multi-Agent Deep Reinforcement Learning for Autonomous Mobility on Demand Systems},
    author = {Hoppe, Heiko and Enders, Tobias and Cappart, Quentin and Schiffer, Maximilian},
    pages = {260--272},
    abstract = {We study vehicle dispatching in autonomous mobility on demand (AMoD) systems, where a central operator assigns vehicles to customer requests or rejects these with the aim of maximizing its total profit. Recent approaches use multi-agent deep reinforcement learning (MADRL) to realize scalable yet performant algorithms, but train agents based on local rewards, which distorts the reward signal with respect to the system-wide profit, leading to lower performance. We therefore propose a novel global-rewards-based MADRL algorithm for vehicle dispatching in AMoD systems, which resolves so far existing goal conflicts between the trained agents and the operator by assigning rewards to agents leveraging a counterfactual baseline. Our algorithm shows statistically significant improvements across various settings on real-world data compared to state-of-the-art MADRL algorithms with local rewards. We further provide a structural analysis which shows that the utilization of global rewards can improve implicit vehicle balancing and demand forecasting abilities. An extended version of our paper, including an appendix, can be found at https://arxiv.org/abs/2312.08884. Our code is available at https://github.com/tumBAIS/GR-MADRL-AMoD.}
}

@InProceedings{gautam41,
    title = {Soft Convex Quantization: Revisiting Vector Quantization with Convex Optimization},
    author = {Gautam, Tanmay and Pryzant, Reid and Yang, Ziyi and Zhu, Chenguang and Sojoudi, Somayeh},
    pages = {273--285},
    abstract = {Vector Quantization (VQ) is a well-known technique in deep learning for extracting informative discrete latent representations. VQ-embedded models have shown impressive results in a range of applications including image and speech generation. VQ operates as a parametric K-means algorithm that quantizes inputs using a single codebook vector in the forward pass. While powerful, this technique faces practical challenges including codebook collapse, non-differentiability and lossy compression. To mitigate the aforementioned issues, we propose Soft Convex Quantization (SCQ) as a direct substitute for VQ. SCQ works like a differentiable convex optimization (DCO) layer: in the forward pass, we solve for the optimal convex combination of codebook vectors to quantize the inputs. In the backward pass, we leverage differentiability through the optimality conditions of the forward solution. We then introduce a scalable relaxation of the SCQ optimization and demonstrate its efficacy on the CIFAR-10, GTSRB and LSUN datasets. We train powerful SCQ autoencoder models that significantly outperform matched VQ architectures, observing an order of magnitude better image reconstruction and codebook usage with comparable quantization runtime.}
}

@InProceedings{tang42,
    title = {Uncertainty Quantification of Set-Membership Estimation in Control and Perception: Revisiting the Minimum Enclosing Ellipsoid},
    author = {Tang, Yukai and Lasserre, Jean-Bernard and Yang, Heng},
    pages = {286--298},
    abstract = {Set-membership estimation (SME) outputs a set estimator that guarantees to cover the groundtruth. Such sets are, however, defined by (many) abstract (and potentially nonconvex) constraints and therefore difficult to manipulate. We present tractable algorithms to compute simple and tight overapproximations of SME in the form of minimum enclosing ellipsoids (MEE). We first introduce the hierarchy of enclosing ellipsoids proposed by Nie and Demmel (2005), based on sums-of-squares relaxations, that asymptotically converge to the MEE of a basic semialgebraic set. This framework, however, struggles in modern control and perception problems due to computational challenges. We contribute three computational enhancements to make this framework practical, namely constraints pruning, generalized relaxed Chebyshev center, and handling non-Euclidean geometry. We showcase numerical examples on system identification and object pose estimation.}
}

@InProceedings{kjellqvist45,
    title = {Minimax dual control with finite-dimensional information state},
    author = {Kjellqvist, Olle},
    pages = {299--311},
    abstract = {This article considers output-feedback control of systems where the function mapping states to measurements has a set-valued inverse. We show that if the set has a bounded number of elements, then minimax dual control of such systems admits finite-dimensional information states. We specialize our results to a discrete-time integrator with magnitude measurements and derive a surprisingly simple sub-optimal control policy that ensures finite gain of the closed loop. The sub-optimal policy is a proportional controller where the magnitude of the gain is computed offline, but the sign is learned, forgotten, and relearned online. The discrete-time integrator with magnitude measurements captures real-world applications such as antenna alignment, and despite its simplicity, it defies established control-design methods. For example, whether a stabilizing linear time-invariant controller exists for this system is unknown, and we conjecture that none exists.}
}

@InProceedings{alsalti46,
    title = {An efficient data-based off-policy {Q}-learning algorithm for optimal output feedback control of linear systems},
    author = {Alsalti, Mohammad and Lopez, Victor G. and M\"{u}ller, Matthias A.},
    pages = {312--323},
    abstract = {In this paper, we present a Q-learning algorithm to solve the optimal output regulation problem for discrete-time LTI systems. This off-policy algorithm only relies on using persistently exciting input-output data, measured offline. No model knowledge or state measurements are needed and the obtained optimal policy only uses past input-output information. Moreover, our formulation of the proposed algorithm renders it computationally efficient. We provide conditions that guarantee the convergence of the algorithm to the optimal solution. Finally, the performance of our method is compared to existing algorithms in the literature.}
}

@InProceedings{wang48,
    title = {Adapting Image-based {RL} Policies via Predicted Rewards},
    author = {Wang, Weiyao and Fang, Xinyuan and Hager, Gregory},
    pages = {324--336},
    abstract = {Image-based reinforcement learning (RL) faces significant challenges in generalization when the visual environment undergoes substantial changes between training and deployment. Under such circumstances, learned policies may not perform well leading to degraded results. Previous approaches to this problem have largely focused on broadening the training observation distribution, employing techniques like data augmentation and domain randomization. However, given the sequential nature of the RL decision-making problem, it is often the case that residual errors are propagated by the learned policy model and accumulate throughout the trajectory, resulting in highly degraded performance. In this paper, we leverage the observation that predicted rewards under domain shift, even though imperfect, can still be a useful signal to guide fine-tuning. We exploit this property to fine-tune a policy using reward prediction in the target domain. We have found that, even under significant domain shift, the predicted reward can still provide meaningful signal and fine-tuning substantially improves the original policy. Our approach, termed Predicted Reward Fine-tuning (PRFT), improves performance across diverse tasks in both simulated benchmarks and real-world experiments. More information is available at project web page: https://sites.google.com/view/prft.}
}

@InProceedings{teichrib49,
    title = {Piecewise regression via mixed-integer programming for {MPC}},
    author = {Teichrib, Dieter and Darup, Moritz Schulze},
    pages = {337--348},
    abstract = {Piecewise regression is a versatile approach used in various disciplines to approximate complex functions from limited, potentially noisy data points. In control, piecewise regression is, e.g., used to approximate the optimal control law of model predictive control (MPC), the optimal value function, or unknown system dynamics. Neural networks are a common choice to solve the piecewise regression problem. However, due to their nonlinear structure, training is often based on gradient-based methods, which may fail to find a global optimum or even a solution that leads to a small approximation error. To overcome this problem and to find a global optimal solution, methods based on mixed-integer programming (MIP) can be used. However, the known MIP-based methods are either limited to a special class of functions, e.g., convex piecewise affine functions, or they lead to complex approximations in terms of the number of regions of the piecewise defined function. Both complicate a usage in the framework of control. We propose a new MIP-based method that is not restricted to a particular class of piecewise defined functions and leads to functions that are fast to evaluate and can be used within an optimization problem, making them well suited for use in control.}
}

@InProceedings{hose50,
    title = {Parameter-Adaptive Approximate {MPC}: Tuning Neural-Network Controllers without Retraining},
    author = {Hose, Henrik and Gr\"{a}fe, Alexander and Trimpe, Sebastian},
    pages = {349--360},
    abstract = {Model Predictive Control (MPC) is a method to control nonlinear systems with guaranteed stability and constraint satisfaction but suffers from high computation times. Approximate MPC (AMPC) with neural networks (NNs) has emerged to address this limitation, enabling deployment on resource-constrained embedded systems. However, when tuning AMPCs for real-world systems, large datasets need to be regenerated and the NN needs to be retrained at every tuning step. This work introduces a novel, parameter-adaptive AMPC architecture capable of online tuning without recomputing large datasets and retraining. By incorporating local sensitivities of nonlinear programs, the proposed method not only mimics optimal MPC inputs but also adjusts to known changes in physical parameters of the model using linear predictions while still guaranteeing stability. We showcase the effectiveness of parameter-adaptive AMPC by controlling the swing-ups of two different real cartpole systems with a severely resource-constrained microcontroller (MCU). We use the same NN across both system instances that have different parameters. This work not only represents the first experimental demonstration of AMPC for fast-moving systems on low-cost MCUs to the best of our knowledge, but also showcases generalization across system instances and variations through our parameter-adaptation method. Taken together, these contributions represent a marked step toward the practical application of AMPC in real-world systems.}
}

@InProceedings{mao51,
    title = {$\widetilde{O}({T}^{-1})$ Convergence to (Coarse) Correlated Equilibria in Full-Information General-Sum {M}arkov Games},
    author = {Mao, Weichao and Qiu, Haoran and Wang, Chen and Franke, Hubertus and Kalbarczyk, Zbigniew and Başar, Tamer},
    pages = {361--374},
    abstract = {No-regret learning has a long history of being closely connected to game theory. Recent works have devised uncoupled no-regret learning dynamics that, when adopted by all the players in normal-form games, converge to various equilibrium solutions at a near-optimal rate of $\widetilde{O}(T^{-1})$, a significant improvement over the $O(1/\sqrt{T})$ rate of classic no-regret learners. However, analogous convergence results are scarce in Markov games, a more generic setting that lays the foundation for multi-agent reinforcement learning. In this work, we close this gap by showing that the optimistic-follow-the-regularized-leader (OFTRL) algorithm, together with appropriate value update procedures, can find $\widetilde{O}(T^{-1})$-approximate (coarse) correlated equilibria in full-information general-sum Markov games within $T$ iterations. Numerical results are also included to corroborate our theoretical findings.}
}

@InProceedings{rickenbach52,
    title = {Inverse Optimal Control as an Errors-in-Variables Problem},
    author = {Rickenbach, Rahel and Scampicchio, Anna and Zeilinger, Melanie N.},
    pages = {375--386},
    abstract = {Inverse optimal control (IOC) is about estimating an unknown objective of interest given its optimal control sequence. However, truly optimal demonstrations are often difficult to obtain, e.g., due to human errors or inaccurate measurements. This paper presents an IOC framework for objective estimation from multiple sub-optimal demonstrations in constrained environments. It builds upon the Karush-Kuhn-Tucker optimality conditions, and addresses the Errors-In-Variables problem that emerges from the use of sub-optimal data. The approach presented is applied to various systems in simulation, and consistency guarantees are provided for linear systems with zero mean additive noise, polytopic constraints, and objectives with quadratic features.}
}

@InProceedings{chatzikiriakos54,
    title = {Learning Soft Constrained {MPC} Value Functions: Efficient {MPC} Design and Implementation providing Stability and Safety Guarantees},
    author = {Chatzikiriakos, Nicolas and Wabersich, Kim Peter and Berkel, Felix and Pauli, Patricia and Iannelli, Andrea},
    pages = {387--398},
    abstract = {Model Predictive Control (MPC) can be applied to safety-critical control problems, providing closed-loop safety and performance guarantees. Application of MPC requires solving an optimization problem at every sampling instant, making it challenging to implement on embedded hardware. To address this challenge, we propose a framework that combines a tightened soft constrained MPC formulation with a supervised learning framework to approximate the MPC value function. This combination enables us to obtain a corresponding optimal control law, which can be implemented efficiently on embedded platforms. The proposed framework ensures stability and constraint satisfaction for various nonlinear systems. While the design effort is similar to the design of nominal MPC formulations, we can establish input-to-state stability (ISS) with respect to the approximation error of the value function. Moreover, we prove that, while the optimal control law may be discontinuous, the value function corresponding to the soft constrained MPC problem is Lipschitz continuous for Lipschitz continuous systems. This serves two purposes: First, it allows to relate approximation errors to a sufficiently large constraint tightening to obtain constraint satisfaction guarantees. Secondly, it enables a very efficient supervised learning procedure for obtaining the approximation using continuous function approximator classes. We showcase the effectiveness of the method through a nonlinear numerical example.}
}

@InProceedings{lu55,
    title = {{MPC}-Inspired Reinforcement Learning for Verifiable Model-Free Control},
    author = {Lu, Yiwen and Li, Zishuo and Zhou, Yihan and Li, Na and Mo, Yilin},
    pages = {399--413},
    abstract = {In this paper, we introduce a new class of parameterized controllers, drawing inspiration from Model Predictive Control (MPC). These controllers adopt an unrolled Quadratic Programming (QP) solver, structured similarly to a deep neural network, with parameters from the QP problem that is similar to linear MPC. The parameters are learned rather than derived from models. This approach addresses the limitations of commonly learned controllers with Multi-Layer Perceptron (MLP) or other general neural network architecture in deep reinforcement learning, in terms of explainability and performance guarantees. The learned controllers not only possess verifiable properties like persistent feasibility and asymptotic stability akin to MPC, but they also empirically match MPC and MLP controllers in control performance. Moreover, they are more computationally efficient in implementation compared to MPC and require significantly fewer learnable policy parameters than MLP controllers. Practical application is demonstrated through a vehicle drift maneuvering task, showcasing the potential of these controllers in real-world scenarios.}
}

@InProceedings{bhardwaj56,
    title = {Real-World Fluid Directed Rigid Body Control via Deep Reinforcement Learning},
    author = {Bhardwaj, Mohak and Lampe, Thomas and Neunert, Michael and Romano, Francesco and Abdolmaleki, Abbas and Byravan, Arunkumar and Wulfmeier, Markus and Riedmiller, Martin and Buchli, Jonas},
    pages = {414--427},
    abstract = {Recent advances in real-world applications of reinforcement learning (RL) have relied on the ability to accurately simulate systems at scale. However, domains such as fluid dynamical systems exhibit complex dynamic phenomena that are hard to simulate at high integration rates, limiting the direct application of modern deep RL algorithms to often expensive or safety critical hardware. In this work, we introduce ``Box o' Flows'', a novel benchtop experimental control system for systematically evaluating RL algorithms in dynamic real-world scenarios. We describe the key components of the Box o' Flows, and through a series of experiments demonstrate how state-of-the-art model-free RL algorithms can synthesize a variety of complex behaviors via simple reward specifications. Furthermore, we explore the role of offline RL in data-efficient hypothesis testing by reusing past experiences. We believe that the insights gained from this preliminary study and the availability of systems like the Box o' Flows support the way forward for developing systematic RL algorithms that can be generally applied to complex, dynamical systems. Supplementary material and videos of experiments are available at https://sites.google.com/view/box-o-flows/home.}
}

@InProceedings{xuan57,
    title = {On the Uniqueness of Solution for the {B}ellman Equation of {LTL} Objectives},
    author = {Xuan, Zetong and Bozkurt, Alper and Pajic, Miroslav and Wang, Yu},
    pages = {428--439},
    abstract = {Surrogate rewards for linear temporal logic (LTL) objectives are commonly utilized in planning problems for LTL objectives. In a widely-adopted surrogate reward approach, two discount factors are used to ensure that the expected return approximates the satisfaction probability of the LTL objective. The expected return then can be estimated by methods using the Bellman updates such as reinforcement learning. However, the uniqueness of the solution to the Bellman equation with two discount factors has not been explicitly discussed. We demonstrate with an example that when one of the discount factors is set to one, as allowed in many previous works, the Bellman equation may have multiple solutions, leading to inaccurate evaluation of the expected return. We then propose a condition for the Bellman equation to have the expected return as the unique solution, requiring the solutions for states inside a rejecting bottom strongly connected component (BSCC) to be $0$. We prove this condition is sufficient by showing that the solutions for the states with discounting can be separated from those for the states without discounting under this condition.}
}

@InProceedings{toufighi61,
    title = {Decision Boundary Learning For Safe Vision-based Navigation via {H}amilton-{J}acobi Reachability Analysis and Support Vector Machine},
    author = {Toufighi, Tara and Bui, Minh and Shrestha, Rakesh and Chen, Mo},
    pages = {440--452},
    abstract = {We develop a self-supervised learning method that can predict safe and unsafe high-level waypoints for robot navigation in the form of a decision boundary given solely a RGB image without knowledge of a prior map. To provide the theoretical basis for such prediction, we use a Hamilton-Jacobi reachability analysis, a formal verification method, as the oracle for labeling training datasets. Given the labeled data, our neural network learn the coefficients of a decision boundary via a soft-margin Support Vector Machine loss function to classify safe and unsafe system states. We experimentally show that our method is generalizable and generates safety decision boundaries in unseen indoor environments. Our method advantages are its explainability and accurate safety prediction, which is important for safety-critical systems. Finally, we demonstrate our method via experiments where we showcase the learning-based safe decision boundary estimation that employs monocular RGB images, and current linear speed.}
}

@InProceedings{wang62,
    title = {Understanding the Difficulty of Solving {C}auchy Problems with {PINN}s},
    author = {Wang, Tao and Zhao, Bo and Gao, Sicun and Yu, Rose},
    pages = {453--465},
    abstract = {Physics-Informed Neural Networks (PINNs) have gained popularity in scientific computing in recent years. However, they often fail to achieve the same level of accuracy as classical methods in solving differential equations. In this paper, we aim to understand this issue from two perspectives in the case of Cauchy problems: the use of $L^2$ residuals as objective functions and the approximation gap of neural networks. We show that minimizing the sum of $L^2$ residual and initial condition error is not sufficient to guarantee the true solution, as this loss function does not capture the underlying dynamics. Additionally, neural networks are not capable of capturing singularities in the solutions due to the non-compactness of their image sets. This, in turn, influences the existence of global minima and the regularity of the network. We demonstrate that when the global minimum does not exist, machine precision becomes the predominant source of achievable error in practice. We also present numerical experiments in support of our theoretical claims.}
}

@InProceedings{ohnishi64,
    title = {Signatures Meet Dynamic Programming: Generalizing {B}ellman Equations for Trajectory Following},
    author = {Ohnishi, Motoya and Akinola, Iretiayo and Xu, Jie and Mandlekar, Ajay and Ramos, Fabio},
    pages = {466--479},
    abstract = {Path signatures have been proposed as a powerful representation of paths that efficiently captures the path's analytic and geometric characteristics, having useful algebraic properties including fast concatenation of paths through tensor products. Signatures have recently been widely adopted in machine learning problems for time series analysis. In this work we establish connections between value functions typically used in optimal control and intriguing properties of path signatures. These connections motivate our novel control framework with signature transforms that efficiently generalizes the Bellman equation to the space of trajectories. We analyze the properties and advantages of the framework, termed signature control. In particular, we demonstrate that (i) it can naturally deal with varying/adaptive time steps; (ii) it propagates higher-level information more efficiently than value function updates; (iii) it is robust to dynamical system misspecification over long rollouts. As a specific case of our framework, we devise a model predictive control method for path tracking. This method generalizes integral control, being suitable for problems with unknown disturbances. The proposed algorithms are tested in simulation, with differentiable physics models including typical control and robotics tasks such as point-mass, curve following for an ant model, and a robotic manipulator.}
}

@InProceedings{hebbar67,
    title = {Online Decision Making with History-Average Dependent Costs},
    author = {Hebbar, Vijeth and Langbort, Cedric},
    pages = {480--491},
    abstract = {In many online sequential decision-making scenarios, a learner's choices affect not just their current costs but also the future ones. In this work, we look at one particular case of such a situation where the costs depend on the time average of past decisions over a history horizon. We first recast this problem with history dependent costs as a problem of decision making under stage-wise constraints. To tackle this, we then propose the novel Follow-The-Adaptively-Regularized-Leader (FTARL) algorithm. Our innovative algorithm incorporates adaptive regularizers that depend explicitly on past decisions, allowing us to enforce stage-wise constraints while simultaneously enabling us to establish tight regret bounds. We also discuss the implications of the length of history horizon on design of no-regret algorithms for our problem and present impossibility results when it is the full learning horizon.}
}

@InProceedings{gao68,
    title = {Learning-based Rigid Tube Model Predictive Control},
    author = {Gao, Yulong and Yan, Shuhao and Zhou, Jian and Cannon, Mark and Abate, Alessandro and Johansson, Karl Henrik},
    pages = {492--503},
    abstract = {This paper is concerned with model predictive control (MPC) of discrete-time linear systems subject to bounded additive disturbance and mixed constraints on the state and input, whereas the true disturbance set is unknown. Unlike most existing work on robust MPC, we propose an algorithm incorporating online learning that builds on prior knowledge of the disturbance, i.e., a known but conservative disturbance set. We approximate the true disturbance set at each time step with a parameterised set, which is referred to as a quantified disturbance set, using disturbance realisations. A key novelty is that the parameterisation of these quantified disturbance sets enjoys desirable properties such that the quantified disturbance set and its corresponding rigid tube bounding disturbance propagation can be efficiently updated online. We provide statistical gaps between the true and quantified disturbance sets, based on which, probabilistic recursive feasibility of MPC optimisation problems is discussed. Numerical simulations are provided to demonstrate the efficacy and computational advantages of our proposed algorithm and compare with conventional robust MPC algorithms.}
}

@InProceedings{rantzer70,
    title = {A Data-driven {R}iccati Equation},
    author = {Rantzer, Anders},
    pages = {504--513},
    abstract = {Certainty equivalence adaptive controllers are analysed using a ``data-driven Riccati equation'', corresponding to the model-free Bellman equation used in Q-learning. The equation depends quadratically on data correlation matrices. This makes it possible to derive simple sufficient conditions for stability and robustness to unmodeled dynamics in adaptive systems. The paper is concluded by short remarks on how the bounds can be used to quantify the interplay between excitation levels and robustness.}
}

@InProceedings{dietrich71,
    title = {Nonconvex Scenario Optimization for Data-Driven Reachability},
    author = {Dietrich, Elizabeth and Devonport, Alex and Arcak, Murat},
    pages = {514--527},
    abstract = {Many of the popular reachability analysis methods rely on the existence of system models. When system dynamics are uncertain or unknown, data-driven techniques must be utilized instead. In this paper, we propose an approach to data-driven reachability that provides a probabilistic guarantee of correctness for these systems through nonconvex scenario optimization. We pose the problem of finding reachable sets directly from data as a chance-constrained optimization problem, and present two algorithms for estimating nonconvex reachable sets: (1) through the union of partition cells and (2) through the sum of radial basis functions. Additionally, we investigate numerical examples to demonstrate the capability and applicability of the introduced methods to provide nonconvex reachable set approximations.}
}

@InProceedings{chee72,
    title = {Uncertainty Quantification and Robustification of Model-based Controllers using Conformal Prediction},
    author = {Chee, Kong Yao and Silva, Thales C. and Hsieh, M. Ani and Pappas, George J.},
    pages = {528--540},
    abstract = {In modern model-based control frameworks such as model predictive control or model-based reinforcement learning, machine learning has become a ubiquitous class of techniques deployed to improve the accuracy of the dynamics models. By leveraging expressive architectures such as neural networks, these frameworks aim to improve both the model accuracy and the control performance of the system, through the construction of accurate data-driven representations of the system dynamics. Despite achieving significant performance improvements over their non-learning counterparts, there are often little or no guarantees on how these model-based controllers with learned models would perform in the presence of uncertainty. In particular, under the influence of modeling errors, noise and exogenous disturbances, it is challenging to ascertain the accuracy of these learned models. In some cases, constraints may even be violated, rendering the controllers unsafe. In this work, we propose a novel framework that can be applied to a large class of model-based controllers and alleviates the above mentioned issues by robustifying the model-based controllers in an online and modular manner, with provable guarantees on the model accuracy and constraint satisfaction. The framework first deploys conformal prediction to generate finite-sample, provably valid uncertainty regions for the dynamics model in a distribution-free manner. These uncertainty regions are incorporated into the constraints through a dynamic constraint tightening procedure. Together with the formulation of a predictive reference generator, a set of robustified reference trajectories are generated and incorporated into the model-based controller. Using two practical case studies, we demonstrate that our proposed methodology not only produces well-calibrated uncertainty regions that establish the accuracy of the models, but also enables the closed-loop system to satisfy constraints in a robust yet non-conservative manner.}
}

@InProceedings{salzmann73,
    title = {Learning for {CasADi}: Data-driven Models in Numerical Optimization},
    author = {Salzmann, Tim and Arrizabalaga, Jon and Andersson, Joel and Pavone, Marco and Ryll, Markus},
    pages = {541--553},
    abstract = {While real-world problems are often challenging to analyze analytically, deep learning excels in modeling complex processes from data. Existing optimization frameworks like CasADi facilitate seamless usage of solvers but face challenges when integrating learned process models into numerical optimizations. To address this gap, we present the Learning for CasADi (L4CasADi) framework, enabling the seamless integration of PyTorch-learned models with CasADi for efficient and potentially hardware-accelerated numerical optimization. The applicability of L4CasADi is demonstrated with two tutorial examples: First, we optimize a fish's trajectory in a turbulent river for energy efficiency where the turbulent flow is represented by a PyTorch model. Second, we demonstrate how an implicit Neural Radiance Field environment representation can be easily leveraged for optimal control with L4CasADi. L4CasADi, along with examples and documentation, is available under MIT license at https://github.com/Tim-Salzmann/l4casadi}
}

@InProceedings{zhang74,
    title = {Neural Operators for Boundary Stabilization of Stop-and-Go Traffic},
    author = {Zhang, Yihuai and Zhong, Ruiguo and Yu, Huan},
    pages = {554--565},
    abstract = {This paper introduces a novel approach to PDE boundary control design using neural operators to alleviate stop-and-go traffic instabilities. Our framework leverages neural operators to design control strategies for traffic flow systems. The traffic dynamics are described by the Aw-Rascle- Zhang (ARZ) model, which consists of second-order coupled hyperbolic partial differential equations (PDEs). The backstepping method which involves constructing and solving a backstepping control kernel is widely used for boundary control of such PDE systems, but it requires intensive depth of expertise and can be time-consuming. To overcome these challenges, we present two distinct neural operator (NO) learning schemes aimed at stabilizing the traffic PDE system. The first scheme embeds NO-approximated gain kernels within a predefined backstepping controller, while the second one directly learns a boundary control law. The Lyapunov analysis is conducted to evaluate the stability of the NO-approximated gain kernels and control law. It is proved that the NO-based closed-loop system is practical stable under certain approximation accuracy conditions. To validate the efficacy of the proposed approach, simulations are conducted to compare the performance of the two neural operator controllers with a PDE backstepping controller and a Proportional Integral (PI) controller. While the NO-approximated methods exhibit larger errors compared to the backstepping controller, they consistently outperform the PI controller, demonstrating faster computation speeds across all scenarios. This result suggests that neural operators can significantly expedite and simplify the process of obtaining boundary controllers for freeway traffic stabilization systems.}
}

@InProceedings{bhargav75,
    title = {Submodular Information Selection for Hypothesis Testing with Misclassification Penalties},
    author = {Bhargav, Jayanth and Ghasemi, Mahsa and Sundaram, Shreyas},
    pages = {566--577},
    abstract = {We consider the problem of selecting an optimal subset of information sources for a hypothesis testing/classification task where the goal is to identify the true state of the world from a finite set of hypotheses, based on finite observation samples from the sources. In order to characterize the learning performance, we propose a misclassification penalty framework, which enables non-uniform treatment of different misclassification errors. In a centralized Bayesian learning setting, we study two variants of the subset selection problem: (i) selecting a minimum cost information set to ensure that the maximum penalty of misclassifying the true hypothesis is below a desired bound and (ii) selecting an optimal information set under a limited budget to minimize the maximum penalty of misclassifying the true hypothesis. Under certain assumptions, we prove that the objective (or constraints) of these combinatorial optimization problems are weak (or approximate) submodular, and establish high-probability performance guarantees for greedy algorithms. Further, we propose an alternate metric for information set selection which is based on the total penalty of misclassification. We prove that this metric is submodular and establish near-optimal guarantees for the greedy algorithms for both the information set selection problems. Finally, we present numerical simulations to validate our theoretical results over several randomly generated instances.}
}

@InProceedings{campanaro76,
    title = {Learning and Deploying Robust Locomotion Policies with Minimal Dynamics Randomization},
    author = {Campanaro, Luigi and Gangapurwala, Siddhant and Merkt, Wolfgang and Havoutis, Ioannis},
    pages = {578--590},
    abstract = {Training Deep Reinforcement Learning (DRL) locomotion policies often require massive amounts of data to converge to the desired behavior. In this regard, simulators provide a cheap and abundant source. For successful sim-to-real transfer, xhaustively engineered approaches such as system identification, dynamics randomization, and domain adaptation are generally employed. As an alternative, we investigate a simple strategy of random force injection (RFI) to perturb system dynamics during training. We show that the application of random forces enables us to emulate dynamics randomization. This allows us to obtain locomotion policies that are robust to variations in system dynamics. We further extend RFI, referred to as extended random force injection (ERFI), by introducing an episodic actuation offset. We demonstrate that ERFI provides additional robustness for variations in system mass offering on average a 53\% improved performance over RFI. We also show that ERFI is sufficient to perform a successful sim-to-real transfer on two different quadrupedal platforms, ANYmal C and Unitree A1, even for perceptive locomotion over uneven terrain in outdoor environments.}
}

@InProceedings{aguiar77,
    title = {Learning flow functions of spiking systems},
    author = {Aguiar, Miguel and Das, Amritam and Johansson, Karl H.},
    pages = {591--602},
    abstract = {We propose a framework for surrogate modelling of spiking systems. These systems are often described by stiff differential equations with high-amplitude oscillations and multi-timescale dynamics, making surrogate models an attractive tool for system design.We parameterise the flow function of a spiking system in state-space using a recurrent neural network architecture, allowing for a direct continuous-time representation of the state trajectories which is particularly advantageous for this class of systems.The spiking nature of the signals makes for a data-heavy and computationally hard training process, and we describe two methods to mitigate these difficulties. We demonstrate our framework on two conductance-based models of biological neurons.}
}

@InProceedings{buerger78,
    title = {Safe Learning in Nonlinear Model Predictive Control},
    author = {Buerger, Johannes and Cannon, Mark and Doff-Sotta, Martin},
    pages = {603--614},
    abstract = {A robust Model Predictive Control algorithm is proposed for learning-based control with model represented by an affine combination of basis functions. The online optimization is formulated as a sequence of convex programming problems derived by linearizing concave components of the dynamic model. A tube-based approach ensures satisfaction of constraints on control variables and model states while avoiding conservative bounds on linearization errors. The linear dependence of the model on unknown parameters is exploited to allow safe online parameter adaptation. The resulting algorithm is recursively feasible and provides closed loop stability and performance guarantees. Numerical examples are provided to illustrate the approach.}
}

@InProceedings{yamada80,
    title = {Efficient Skill Acquisition for Insertion Tasks in Obstructed Environments},
    author = {Yamada, Jun and Collins, Jack and Posner, Ingmar},
    pages = {615--627},
    abstract = {Data efficiency in robotic skill acquisition is crucial for operating robots in varied small-batch assembly settings. To operate in such environments, robots must have robust obstacle avoidance and versatile goal conditioning acquired from only a few simple demonstrations. Existing approaches, however, fall short of these requirements. Deep reinforcement learning (RL) enables a robot to learn complex manipulation tasks but is often limited to small task spaces in the real world due to sample inefficiency and safety concerns. Motion planning (MP) can generate collision-free paths in obstructed environments, but cannot solve complex manipulation tasks and requires goal states often specified by a user or object-specific pose estimator. In this work, we propose a robust system for efficient skill acquisition designed to address complex insertion tasks in obstructed environments. Our system leverages an object-centric generative model (OCGM) for versatile goal identification to specify a goal for MP combined with RL to solve complex manipulation tasks in obstructed environments. Particularly, OCGM enables one-shot target object identification and re-identification in new scenes, allowing MP to guide the robot to the target object while avoiding obstacles. This is combined with a skill transition network, which bridges the gap between terminal states of MP and feasible start states of a sample-efficient RL policy. The experiments demonstrate that our OCGM-based one-shot goal identification provides competitive accuracy to other baseline approaches and that our modular framework outperforms competitive baselines, including a state-of-the-art RL algorithm, by a significant margin for complex manipulation tasks in obstructed environments.}
}

@InProceedings{tian81,
    title = {Balanced Reward-inspired Reinforcement Learning for Autonomous Vehicle Racing},
    author = {Tian, Zhen and Zhao, Dezong and Lin, Zhihao and Flynn, David and Zhao, Wenjing and Tian, Daxin},
    pages = {628--640},
    abstract = {Autonomous vehicle racing has attracted extensive interest due to its great potential in autonomous driving at the extreme limits. Model-based and learning-based methods are being widely used in autonomous racing. However, model-based methods cannot cope with the dynamic environments when only local perception is available. As a comparison, learning-based methods can handle complex environments under local perception. Recently, deep reinforcement learning (DRL) has gained popularity in autonomous racing. DRL outperforms conventional learning- based methods by handling complex situations and leveraging local information. DRL algorithms, such as the proximal policy algorithm, can achieve a good balance between the execution time and safety in autonomous vehicle competition. However, the training outcomes of conventional DRL methods exhibit inconsistent correctness in decision-making. The instability in decision-making introduces safety concerns in autonomous vehicle racing, such as collisions into track boundaries. The proposed algorithm is capable to avoid collisions and improve the training quality. Simulation results on a physical engine demonstrate that the proposed algorithm outperforms other DRL algorithms in achieving safer control during sharp bends, fewer collisions into track boundaries, and higher training quality among multiple tracks.}
}

@InProceedings{zhang82,
    title = {An Invariant Information Geometric Method for High-Dimensional Online Optimization},
    author = {Zhang, Zhengfei and Wei, Yunyue and Sui, Yanan},
    pages = {641--653},
    abstract = {Sample efficiency is crucial in optimization, particularly in black-box scenarios characterized by expensive evaluations and zeroth-order feedback. When computing resources are plentiful, Bayesian optimization is often favored over evolution strategies. In this paper, we introduce a full invariance oriented evolution strategies algorithm, derived from its corresponding framework, that effectively rivals the leading Bayesian optimization method in tasks with dimensions at the upper limit of Bayesian capability. Specifically, we first build the framework InvIGO that fully incorporates historical information while retaining the full invariant and computational complexity. We then exemplify InvIGO on multi-dimensional Gaussian, which gives an invariant and scalable optimizer SynCMA . The theoretical behavior and advantages of our algorithm over other Gaussian-based evolution strategies are further analyzed. Finally, We benchmark SynCMA against leading algorithms in Bayesian optimization and evolution strategies on various high dimension tasks, including Mujoco locomotion tasks, rover planning task and synthetic functions. In all scenarios, SynCMA demonstrates great competence, if not dominance, over other algorithms in sample efficiency, showing the underdeveloped potential of property oriented evolution strategies.}
}

@InProceedings{han83,
    title = {On the Nonsmooth Geometry and Neural Approximation of the Optimal Value Function of Infinite-Horizon Pendulum Swing-up},
    author = {Han, Haoyu and Yang, Heng},
    pages = {654--666},
    abstract = {We revisit the inverted pendulum problem with the goal of understanding and computing the true optimal value function. We start with an observation that the true optimal value function must be nonsmooth (i.e., not globally C1) due to symmetry of the problem. We then give a result that can certify the optimality of a candidate piece-wise C1 value function. Further, for a candidate value function obtained via numerical approximation, we provide a bound of suboptimality based on its Hamilton-Jacobi-Bellman (HJB) equation residuals. Inspired by Holzh\"{u}ter (2004), we then design an algorithm that solves backwards the Pontryagin's minimum principle (PMP) ODE from terminal conditions provided by the locally optimal LQR value function. This numerical procedure leads to a piece-wise C1 value function whose nonsmooth region contains periodic spiral lines and smooth regions attain HJB residuals about $10^{-4}$, hence certiﬁed to be the optimal value function up to minor numerical inaccuracies. This optimal value function checks the power of optimality: (i) it sits above a polynomial lower bound; (ii) its induced controller globally swings up and stabilizes the pendulum, and (iii) attains lower trajectory cost than baseline methods such as energy shaping, model predictive control (MPC), and proximal policy optimization (with MPC attaining almost the same cost). We conclude by distilling the optimal value function into a simple neural network.}
}

@InProceedings{pilipovsky85,
    title = {Data-Driven Robust Covariance Control for Uncertain Linear Systems},
    author = {Pilipovsky, Joshua and Tsiotras, Panagiotis},
    pages = {667--678},
    abstract = {The theory of covariance control and covariance steering (CS) deals with controlling the dispersion of trajectories of a dynamical system, under the implicit assumption that accurate prior knowledge of the system being controlled is available. In this work, we consider the problem of steering the distribution of a discrete-time, linear system subject to exogenous disturbances under an unknown dynamics model. Leveraging concepts from behavioral systems theory, the trajectories of this unknown, noisy system may be (approximately) represented using system data collected through experimentation. Using this fact, we formulate a direct data-driven covariance control problem using input-state data. We then propose a maximum likelihood uncertainty quantification method to estimate and bound the noise realizations in the data collection process. Lastly, we utilize robust convex optimization techniques to solve the resulting norm-bounded uncertain convex program. We illustrate the proposed end-to-end data-driven CS algorithm on a double integrator example and showcase the efficacy and accuracy of the proposed method compared to that of model-based methods.}
}

@InProceedings{shen86,
    title = {Combining Model-based Controller and {ML} Advice via Convex Reparameterization},
    author = {Shen, Junxuan and Wierman, Adam and Qu, Guannan},
    pages = {679--693},
    abstract = {Machine Learning (ML) based control, particularly Reinforcement Learning (RL), has achieved impressive advancements but is often black-box and lacks worst-case guarantees in safety-critical systems. In contrast, classical model-based control offers stability guarantees but usually underperforms the machine-learned black-box controller. This motivates us to combine machine-learned black-box and model-based controllers. Due to the nonconvexity of the space of stable controllers, a simple convex combination of the two controllers can lead to instability. We propose using Disturbance Response Control (DRC) to reparameterize the two controllers, ensuring the convexity of the stable controller space. We then propose lambdaCLEAC, which adaptively combines the machine-learned black-box controller and the model-based controller in the DRC parameterization. We prove that our approach achieves the best of both worlds: stability as in model-based control and similar regret bounds as the machine-learned controller.}
}

@InProceedings{brindise87,
    title = {Pointwise-in-Time Diagnostics for Reinforcement Learning During Training and Runtime},
    author = {Brindise, Noel and Moreno, Andres Posada and Langbort, Cedric and Trimpe, Sebastian},
    pages = {694--706},
    abstract = {Explainable AI Planning (XAIP), a subfield of xAI, offers a variety of methods to interpret the behavior of autonomous systems. A recent ``pointwise-in-time'' explanation method, called Rule Status Assessment (RSA), characterizes an agent's behavior at individual time steps in a trajectory using linear temporal logic (LTL) rules. In this work, RSA is applied for the first time in a reinforcement learning (RL) context. We first demonstrate RSA diagnostics as a substantial supplement to the basic RL reward curve, tracking whether and when specified subtasks are accomplished. We then introduce a novel ``Interactive RSA'' which provides the user with detailed diagnostic information automatically at any desired point in a trajectory. We apply RSA to an advanced agent at runtime and show that RSA and its novel interactive variant constitute a promising step towards explainable RL.}
}

@InProceedings{zhou88,
    title = {Expert with Clustering: Hierarchical Online Preference Learning Framework},
    author = {Zhou, Tianyue and Cho, Jung-Hoon and Ardabili, Babak Rahimi and Tabkhi, Hamed and Wu, Cathy},
    pages = {707--718},
    abstract = {Emerging mobility systems are increasingly capable of recommending options to mobility users, to guide them towards personalized yet sustainable system outcomes. Even more so than the typical recommendation system, it is crucial to minimize regret, because 1) the mobility options directly affect the lives of the users, and 2) the system sustainability relies on sufficient user participation. In this study, we thus consider accelerating user preference learning by exploiting a low-dimensional latent space that captures the mobility preferences of users within a population. We therefore introduce a hierarchical contextual bandit framework named Expert with Clustering (EWC), which integrates clustering techniques and prediction with expert advice. EWC efficiently utilizes hierarchical user information and incorporates a novel Loss-guided Distance metric. This metric is instrumental in generating more representative cluster centroids, thereby enhancing the performance of recommendation systems. In a recommendation scenario with \(N\) users, \(T\) rounds per user, and \(K\) options, our algorithm achieves a regret bound of \(O(N\sqrt{T\log K} + NT)\). This bound consists of two parts: the first term is the regret from the Hedge algorithm, and the second term depends on the average loss from clustering. The algorithm performs with low regret, especially when a latent hierarchical structure exists among users. This regret bound underscores the theoretical and experimental efficacy of EWC, particularly in scenarios that demand rapid learning and adaptation. Experimental results highlight that EWC can substantially reduce regret by 27.57\% compared to the LinUCB baseline. Our work offers a data-efficient approach to capturing both individual and collective behaviors, making it highly applicable to contexts with hierarchical structures. We expect the algorithm to be applicable to other settings with layered nuances of user preferences and information.}
}

@InProceedings{lin90,
    title = {Verification of Neural Reachable Tubes via Scenario Optimization and Conformal Prediction},
    author = {Lin, Albert and Bansal, Somil},
    pages = {719--731},
    abstract = {Learning-based approaches for controlling safety-critical autonomous systems are rapidly growing in popularity; thus, it is important to provide rigorous and robust assurances on their performance and safety. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for providing such guarantees, since it can handle general nonlinear system dynamics, bounded adversarial system disturbances, and state and input constraints. However, it involves solving a Partial Differential Equation (PDE), whose computational and memory complexity scales exponentially with respect to the state dimension, making its direct use on large-scale systems intractable. To overcome this challenge, neural approaches, such as DeepReach, have been used to synthesize reachable tubes and safety controllers for high-dimensional systems. However, verifying these neural reachable tubes remains challenging. In this work, we propose two different verification methods, based on robust scenario optimization and conformal prediction, to provide probabilistic safety guarantees for neural reachable tubes. Our methods allow a direct trade-off between resilience to outlier errors in the neural tube, which are inevitable in a learning-based approach, and the strength of the probabilistic safety guarantee. Furthermore, we show that split conformal prediction, a widely used method in the machine learning community for uncertainty quantification, reduces to a scenario-based approach, making the two methods equivalent not only for verification of neural reachable tubes but also more generally. To our knowledge, our proof is the first in the literature to show a strong relationship between the highly related but disparate fields of conformal prediction and scenario optimization. Finally, we propose an outlier-adjusted verification approach that harnesses information about the error distribution in neural reachable tubes to recover greater safe volumes. We demonstrate the efficacy of the proposed approaches for the high-dimensional problems of multi-vehicle collision avoidance and rocket landing with no-go zones.}
}

@InProceedings{kazemian91,
    title = {Random Features Approximation for Control-Affine Systems},
    author = {Kazemian, Kimia and Sattar, Yahya and Dean, Sarah},
    pages = {732--744},
    abstract = {Modern data-driven control applications call for flexible nonlinear models that are amenable to principled controller synthesis and realtime feedback. Many nonlinear dynamical systems of interest are control affine. We propose two novel classes of nonlinear feature representations which capture control affine structure while allowing for arbitrary complexity in the state dependence. Our methods make use of random features (RF) approximations, inheriting the expressiveness of kernel methods at a lower computational cost. We formalize the representational capabilities of our methods by showing their relationship to the Affine Dot Product (ADP) kernel proposed by Castaneda et al. (2021) and a novel Affine Dense (AD) kernel that we introduce. We further illustrate the utility by presenting a case study of data-driven optimization-based control using control certificate functions (CCF). Simulation experiments on a double pendulum empirically demonstrate the advantages of our methods.}
}

@InProceedings{gibson92,
    title = {Hacking Predictors Means Hacking Cars: Using Sensitivity Analysis to Identify Trajectory Prediction Vulnerabilities for Autonomous Driving Security},
    author = {Gibson, Marsalis and Babazadeh, David and Tomlin, Claire and Sastry, Shankar},
    pages = {745--757},
    abstract = {Adversarial attacks on learning-based multi-modal trajectory predictors have already been demonstrated. However, there are still open questions about the effects of perturbations on inputs other than state histories, and how these attacks impact downstream planning and control. In this paper, we conduct a sensitivity analysis on two trajectory prediction models, Trajectron++ and AgentFormer. The analysis reveals that between all inputs, almost all of the perturbation sensitivities for both models lie only within the most recent position and velocity states. We additionally demonstrate that, despite dominant sensitivity on state history perturbations, an undetectable image map perturbation made with the Fast Gradient Sign Method can induce large prediction error increases in both models, revealing that these trajectory predictors are, in fact, susceptible to image-based attacks. Using an optimization-based planner and example perturbations crafted from sensitivity results, we show how these attacks can cause a vehicle to come to a sudden stop from moderate driving speeds.}
}

@InProceedings{hanson93,
    title = {{R}ademacher Complexity of Neural {ODE}s via {C}hen-{F}liess Series},
    author = {Hanson, Joshua and Raginsky, Maxim},
    pages = {758--769},
    abstract = {We show how continuous-depth neural ODE models can be framed as single-layer, infinite-width nets using the Chen-Fliess series expansion for nonlinear ODEs. In this net, the output ``weights'' are taken from the signature of the control input --- a tool used to represent infinite-dimensional paths as a sequence of tensors --- which comprises iterated integrals of the control input over a simplex. The ``features'' are taken to be iterated Lie derivatives of the output function with respect to the vector fields in the controlled ODE model. The main result of this work applies this framework to derive compact expressions for the Rademacher complexity of ODE models that map an initial condition to a scalar output at some terminal time. The result leverages the straightforward analysis afforded by single-layer architectures. We conclude with some examples instantiating the bound for some specific systems and discuss potential follow-up work.}
}

@InProceedings{zaman95,
    title = {Robust Cooperative Multi-Agent Reinforcement Learning: A Mean-Field Type Game Perspective},
    author = {Zaman, Muhammad Aneeq Uz and Lauri\`{e}re, Mathieu and Koppel, Alec and Ba\c{s}ar, Tamer},
    pages = {770--783},
    abstract = {In this paper, we study the problem of robust cooperative multi-agent reinforcement learning (RL) where a large number of cooperative agents with distributed information aim to learn policies in the presence of stochastic and non-stochastic uncertainties whose distributions are respectively known and unknown. Focusing on policy optimization that accounts for both types of uncertainties, we formulate the problem as a worst-case (minimax) framework. Since this problem is intractable in general, we focus on the Linear Quadratic setting to enable derive benchmark solutions. First, since no standard theory exists for this problem due to the distributed information structure, we utilize the Mean-Field Type Game (MFTG) paradigm to establish guarantees on the solution quality in the sense of achieved Nash equilibrium of the MFTG. This in turn allows us to compare the performance against the corresponding original robust multi-agent control problem. Then, we propose a Receding-horizon Gradient Descent Ascent RL algorithm to find the MFTG Nash equilibrium and we prove a non-asymptotic rate of convergence. Finally, we provide numerical experiments to demonstrate the efficacy of our approach relative to a baseline algorithm.}
}

@InProceedings{qin96,
    title = {Learning $\epsilon$-{N}ash Equilibrium Stationary Policies in Stochastic Games with Unknown Independent Chains Using Online Mirror Descent},
    author = {Qin, Tiancheng and Etesami, S. Rasoul},
    pages = {784--795},
    abstract = {We study a subclass of n-player stochastic games, namely, stochastic games with independent chains and unknown transition matrices. In this class of games, players control their own internal Markov chains whose transitions do not depend on the states/actions of other players. However, players' decisions are coupled through their payoff functions. We assume players can receive only realizations of their payoffs, and that the players can not observe the states and actions of other players, nor do they know the transition probability matrices of their own Markov chain. Relying on a compact dual formulation of the game based on occupancy measures and the technique of confidence set to maintain high-probability estimates of the unknown transition matrices, we propose a fully decentralized mirror descent algorithm to learn an $\epsilon$-Nash equilibrium stationary policy for this class of games. The proposed algorithm has the desired properties of independence and convergence. Specifically, assuming the existence of a variationally stable Nash equilibrium policy, we show that the proposed algorithm in which players make their decisions independently and in a decentralized fashion converges asymptotically to the stable $\epsilon$-Nash equilibrium stationary policy with arbitrarily high probability.}
}

@InProceedings{gupta100,
    title = {Uncertainty Informed Optimal Resource Allocation with {G}aussian Process based Bayesian Inference},
    author = {Gupta, Samarth and Amin, Saurabh},
    pages = {796--812},
    abstract = {We focus on the problem of uncertainty informed allocation of medical resources (vaccines) to heterogeneous populations for managing epidemic spread. We tackle two related questions: (1) For a compartmental ordinary differential equation (ODE) model of epidemic spread, how can we estimate and integrate parameter uncertainty into resource allocation decisions? (2) How can we computationally handle both nonlinear ODE constraints and parameter uncertainties for a generic stochastic optimization problem for resource allocation? To the best of our knowledge current literature does not fully resolve these questions. Here, we develop a data-driven approach to represent parameter uncertainty accurately and tractably in a novel stochastic optimization problem formulation. We first generate a tractable scenario set by estimating the distribution on ODE model parameters using Bayesian inference with Gaussian processes. Next, we develop a parallelized solution algorithm that accounts for scenario-dependent nonlinear ODE constraints. Our scenario-set generation procedure and solution approach are flexible in that they can handle any compartmental epidemiological ODE model. Our computational experiments on two different non-linear ODE models (SEIR and SEPIHR) indicate that accounting for uncertainty in key epidemiological parameters can improve the efficacy of time-critical allocation decisions by 4-8\%. This improvement can be attributed to data-driven and optimal (strategic) nature of vaccine allocations.}
}

@InProceedings{yi101,
    title = {Improving sample efficiency of high dimensional {B}ayesian optimization with {MCMC}},
    author = {Yi, Zeji and Wei, Yunyue and Cheng, Chu Xin and He, Kaibo and Sui, Yanan},
    pages = {813--824},
    abstract = {Sequential optimization methods are often confronted with the curse of dimensionality in high-dimensional spaces. Current approaches under the Gaussian process framework are still burdened by the computational complexity of tracking Gaussian process posteriors and need to partition the optimization problem into small regions to ensure exploration or assume an underlying low-dimensional structure. With the idea of transiting the candidate points towards more promising positions, we propose a new method based on Markov Chain Monte Carlo to efficiently sample from an approximated posterior. We provide theoretical guarantees of its convergence in the Gaussian process Thompson sampling setting. We also show experimentally that both the Metropolis-Hastings and the Langevin Dynamics version of our algorithm outperform state-of-the-art methods in high-dimensional sequential optimization and reinforcement learning benchmarks.}
}

@InProceedings{srinivasan102,
    title = {{SpOiLer}: Offline Reinforcement Learning using Scaled Penalties},
    author = {Srinivasan, Padmanaba and Knottenbelt, William J.},
    pages = {825--838},
    abstract = {Offline Reinforcement Learning (RL) is a variant of off-policy learning where an optimal policy must be learned from a static dataset containing trajectories collected by an unknown behavior policy. In the offline setting, standard off-policy algorithms will overestimate values of out-of-distribution actions and a policy trained naively in this way will perform poorly in the environment due to distribution shift between the implied and real environment; this is especially likely when modelling complex and multi-modal data distributions. We propose Scaled-penalty Offline Learning (SpOiLer), an offline reinforcement learning algorithm that reduces the value of out-of-distribution actions relative to observed actions. The resultant pessimistic value function is a lower bound of the true value function and manipulates the policy towards selecting actions present in the dataset. Our method is a simple augmentation to the standard Bellman backup operator and implementation requires around 15 additional lines of code over soft actor-critic. We provide theoretical insights into how SpOiLer operates under the hood and show empirically that SpOiLer achieves remarkable performance against prior methods on a range of tasks.}
}

@InProceedings{lubsen104,
    title = {Towards Safe Multi-Task {B}ayesian {O}ptimization},
    author = {L\"{u}bsen, Jannis and Hespe, Christian and Eichler, Annika},
    pages = {839--851},
    abstract = {Bayesian optimization has emerged as a highly effective tool for the safe online optimization of systems, due to its high sample efficiency and noise robustness. To further enhance its efficiency, reduced physical models of the system can be incorporated into the optimization process, accelerating it. These models are able to offer an approximation of the actual system, and evaluating them is significantly cheaper. The similarity between the model and reality is represented by additional hyperparameters, which are learned within the optimization process. Safety is a crucial criterion for online optimization methods such as Bayesian optimization, which has been addressed by recent works that provide safety guarantees under the assumption of known hyperparameters. In practice, however, this does not apply. Therefore, we extend the robust Gaussian process uniform error bounds to meet the multi-task setting, which involves the calculation of a confidence region from the hyperparameter posterior distribution utilizing Markov chain Monte Carlo methods. Subsequently, the robust safety bounds are employed to facilitate the safe optimization of the system, while incorporating measurements of the models. Simulation results indicate that the optimization can be significantly accelerated for expensive to evaluate functions in comparison to other state-of-the-art safe Bayesian optimization methods, contingent on the fidelity of the models.}
}

@InProceedings{bai105,
    title = {Mixing Classifiers to Alleviate the Accuracy-Robustness Trade-Off},
    author = {Bai, Yatong and Anderson, Brendon G. and Sojoudi, Somayeh},
    pages = {852--865},
    abstract = {Deep neural classifiers have recently found tremendous success in data-driven control systems. However, existing neural models often suffer from a trade-off between accuracy and adversarial robustness, which is a limitation that must be overcome in the control of safety-critical systems that require both high performance and rigorous robustness guarantees. In this work, we develop classifiers that simultaneously inherit high robustness from robust models and high accuracy from standard models. Specifically, we propose a theoretically motivated formulation that mixes the output probabilities of a standard neural network and a robust neural network. Both of these base classifiers are pre-trained, and thus our method does not require additional training. Our numerical experiments verify that the mixed classifier noticeably improves the accuracy-robustness trade-off and identify the confidence property of the robust base classifier as the key leverage of this more benign trade-off. Our theoretical results prove that under mild assumptions, when the robustness of the robust base model is certifiable, no alteration or attack within a closed-form $l_p$ radius on an input can result in misclassification of the mixed classifier.}
}

@InProceedings{thangavel107,
    title = {Design of observer-based finite-time control for inductively coupled power transfer system with random gain fluctuations},
    author = {Thangavel, Satheesh and Rathinasamy, Sakthivel},
    pages = {866--875},
    abstract = {This investigation focuses on the issues of finite-time stochastic stabilisation and non fragile control design for inductively coupled power transfer systems (ICPTSs) in the presence of stochastic disturbances. Primarily, the observer system exploits the information obtained from the output of the ICPTSs to accurately reconstruct the states of the ICPTS. The observer-based non fragile control is put forward by including the estimated states of the system and gain fluctuations, which assist in achieving the desired finite-time stochastic stabilisation of the addressed system. Furthermore, via the use of Lyapunov stability theory and Ito's formula, conditions based on linear matrix inequalities are derived, which serve as adequate criteria for affirming the desired results. In conclusion, the results of the simulation that have been offered provide evidence that the proposed theoretical outcomes and control system are viable propositions.}
}

@InProceedings{rickard108,
    title = {Learning Robust Policies for Uncertain Parametric {M}arkov Decision Processes},
    author = {Rickard, Luke and Abate, Alessandro and Margellos, Kostas},
    pages = {876--889},
    abstract = {Synthesising verifiably correct controllers for dynamical systems is crucial for safety-critical problems. To achieve this, it is important to account for uncertainty in a robust manner, while at the same time it is often of interest to avoid being overly conservative with the view of achieving a better cost. We propose a method for verifiably safe policy synthesis for a class of finite state models, under the presence of structural uncertainty. In particular, we consider uncertain parametric Markov decision processes (upMDPs), a special class of Markov decision processes, with parameterised transition functions, where such parameters are drawn from a (potentially) unknown distribution. Our framework leverages recent advancements in the so-called scenario approach theory, where we represent the uncertainty by means of scenarios, and provide guarantees on synthesised policies satisfying probabilistic computation tree logic (PCTL) formulae. We consider several common benchmarks/problems and compare our work to recent developments for verifying upMDPs.}
}

@InProceedings{mao109,
    title = {Conditions for Parameter Unidentifiability of Linear {ARX} Systems for Enhancing Security},
    author = {Mao, Xiangyu and He, Jianping and Yu, Chengpu and Fang, Chongrong},
    pages = {890--901},
    abstract = {For an adversarial observer of parametric systems, the identifiability of parameters reflects the possibility of inferring the system dynamics and then affects the performance of attacks against the systems. Hence, achieving unidentifiability of the parameters, which makes the adversary unable to get identification with low variance, is an attractive way to enhance security. In this paper, we propose a quantitative definition to measure the unidentifiability based on the lower bound of identification variance. The lower bound is given via the analysis of the Fisher Information Matrix (FIM). Then, we propose the necessary and sufficient condition for unidentifiability and derive the explicit form of the unidentifiability condition for linear autoregressive systems with exogenous inputs (ARX systems). It is proved that the unidentifiability of linear ARX systems can be achieved through quadratic constraints on inputs and outputs. Finally, considering an optimal control problem with security concerns, we apply the unidentifiability constraint and obtain the optimal controller. Simulations demonstrate the effectiveness of our method.}
}

@InProceedings{toso111,
    title = {Meta-Learning Linear Quadratic Regulators: A Policy Gradient {MAML} Approach for Model-free {LQR}},
    author = {Toso, Leonardo Felipe and Zhan, Donglin and Anderson, James and Wang, Han},
    pages = {902--915},
    abstract = {We investigate the problem of learning linear quadratic regulators (LQR) in a multi-task, heterogeneous, and model-free setting. We characterize the stability and personalization guarantees of a policy gradient-based (PG) model-agnostic meta-learning (MAML) (Finn et al., 2017) approach for the LQR problem under different task-heterogeneity settings. We show that our MAML-LQR algorithm produces a stabilizing controller close to each task-specific optimal controller up to a task-heterogeneity bias in both model-based and model-free learning scenarios. Moreover, in the model-based setting, we show that such a controller is achieved with a linear convergence rate, which improves upon sub-linear rates from existing work. Our theoretical guarantees demonstrate that the learned controller can efficiently adapt to unseen LQR tasks.}
}

@InProceedings{jongeneel114,
    title = {A Large Deviations Perspective on Policy Gradient Algorithms},
    author = {Jongeneel, Wouter and Kuhn, Daniel and Li, Mengmeng},
    pages = {916--928},
    abstract = {Motivated by policy gradient methods in the context of reinforcement learning, we derive the first large deviation rate function for the iterates generated by stochastic gradient descent for possibly non-convex objectives satisfying a Polyak-{\L}ojasiewicz condition. Leveraging the contraction principle from large deviations theory, we illustrate the potential of this result by showing how convergence properties of policy gradient with a softmax parametrization and an entropy regularized objective can be naturally extended to a wide spectrum of other policy parametrizations.}
}

@InProceedings{peralez116,
    title = {Deep model-free {KKL} observer: A switching approach},
    author = {Peralez, Johan and Nadri, Madiha},
    pages = {929--940},
    abstract = {This paper presents a new model-free methodology to learn Kazantzis-Kravaris-Luenberger (KKL) observers for nonlinear systems. We address three major difficulties arising in observer design: the peaking phenomenon, the noise sensitivity and the trade-off between convergence speed and robustness. We formulate the learning objective as an optimization problem, strictly minimizing the error of the observer estimates, without the need of adding explicit constraints or regularization terms. We further improve the performance with a switching approach, efficiently transitioning between two observers, respectively designed for the transient phase and the asymptotic convergence. Numerical results on the Van der Pol system, the R\"{o}ssler attractor and on a bioreactor illustrate the gain of the method regarding the literature, in term of performance and robustness. Code available online: https://github.com/jolindien-git/DeepKKL}
}

@InProceedings{brancato120,
    title = {In vivo learning-based control of microbial populations density in bioreactors},
    author = {Brancato, Sara Maria and Salzano, Davide and Lellis, Francesco De and Fiore, Davide and Russo, Giovanni and Bernardo, Mario di},
    pages = {941--953},
    abstract = {A key problem in using microorganisms as bio-factories is achieving and maintaining cellular communities at the desired density and composition to efficiently convert their biomass into useful compounds. Bioreactors are promising technological platforms for the real-time, scalable control of cellular density. In this work, we developed a learning-based strategy to expand the range of available control algorithms capable of regulating the density of a single bacterial population in bioreactors. Specifically, we used a sim-to-real paradigm, where a simple mathematical model, calibrated using a single experiment, was adopted to generate synthetic data for training the controller. The resulting policy was then exhaustively tested in vivo using a low-cost bioreactor known as Chi.Bio, assessing performance and robustness. Additionally, we compared the performance with more traditional controllers (namely, a PI and an MPC), confirming that the learning-based controller exhibits similar performance in vivo. Our work demonstrates the viability of learning-based strategies for controlling cellular density in bioreactors, making a step forward toward their use in controlling the composition of microbial consortia.}
}

@InProceedings{jarneornia121,
    title = {Bounded Robustness in Reinforcement Learning via Lexicographic Objectives},
    author = {Jarne Ornia, Daniel and Romao, Licio and Hammond, Lewis and Jr, Manuel Mazo and Abate, Alessandro},
    pages = {954--967},
    abstract = {Policy robustness in Reinforcement Learning may not be desirable at any cost: the alterations caused by robustness requirements from otherwise optimal policies should be explainable, quantifiable and formally verifiable. In this work we study how policies can be maximally robust to arbitrary observational noise by analysing how they are altered by this noise through a stochastic linear operator interpretation of the disturbances, and establish connections between robustness and properties of the noise kernel and of the underlying MDPs. Then, we construct sufficient conditions for policy robustness, and propose a robustness-inducing scheme, applicable to any policy gradient algorithm, that formally trades off expected policy utility for robustness through lexicographic optimisation, while preserving convergence and sub-optimality in the policy synthesis.}
}

@InProceedings{li123,
    title = {System-level Safety Guard: Safe Tracking Control through Uncertain Neural Network Dynamics Models},
    author = {Li, Xiao and Li, Yutong and Girard, Anouck and Kolmanovsky, Ilya},
    pages = {968--979},
    abstract = {The Neural Network (NN), as a black-box function approximator, has been considered in many control and robotics applications. However, difficulties in verifying the overall system safety in the presence of uncertainties hinder the deployment of NN modules in safety-critical systems. In this paper, we leverage the NNs as predictive models for trajectory tracking of unknown dynamical systems. We consider controller design in the presence of both intrinsic uncertainty and uncertainties from other system modules. In this setting, we formulate the constrained trajectory tracking problem and show that it can be solved using Mixed-integer Linear Programming (MILP). The proposed MILP-based approach is empirically demonstrated in robot navigation and obstacle avoidance through simulations. The demonstration videos are available at https://xiaolisean.github.io/publication/2023-11-01-L4DC2024.}
}

@InProceedings{lee124,
    title = {Nonasymptotic Regret Analysis of Adaptive Linear Quadratic Control with Model Misspecification},
    author = {Lee, Bruce and Rantzer, Anders and Matni, Nikolai},
    pages = {980--992},
    abstract = {The strategy of pre-training a large model on a diverse dataset, then fine-tuning for a particular application has yielded impressive results in computer vision, natural language processing, and robotic control. This strategy has vast potential in adaptive control, where it is necessary to rapidly adapt to changing conditions with limited data. Toward concretely understanding the benefit of pre-training for adaptive control, we study the adaptive linear quadratic control problem in the setting where the learner has prior knowledge of a collection of basis matrices for the dynamics. This basis is misspecified in the sense that it cannot perfectly represent the dynamics of the underlying data generating process. We propose an algorithm that uses this prior knowledge, and prove upper bounds on the expected regret after $T$ interactions with the system. In the regime where $T$ is small, the upper bounds are dominated by a term scales with either $\texttt{poly}(\log T)$ or $\sqrt{T}$, depending on the prior knowledge available to the learner. When $T$ is large, the regret is dominated by a term that grows with $\delta T$, where $\delta$ quantifies the level of misspecification. This linear term arises due to the inability to perfectly estimate the underlying dynamics using the misspecified basis, and is therefore unavoidable unless the basis matrices are also adapted online. However, it only dominates for large $T$, after the sublinear terms arising due to the error in estimating the weights for the basis matrices become negligible. We provide simulations that validate our analysis. Our simulations also show that offline data from a collection of related systems can be used as part of a pre-training stage to estimate a misspecified dynamics basis, which is in turn used by our adaptive controller.}
}

@InProceedings{liao127,
    title = {Error bounds, {PL} condition, and quadratic growth for weakly convex functions, and linear convergences of proximal point methods},
    author = {Liao, Feng-Yi and Ding, Lijun and Zheng, Yang},
    pages = {993--1005},
    abstract = {Many machine learning problems lack strong convexity properties. Fortunately, recent studies have revealed that first-order algorithms also enjoy linear convergences under various weaker regularity conditions. While the relationship among different conditions for convex and smooth functions is well understood, it is not the case for the nonsmooth setting. In this paper, we go beyond convexity and smoothness, and clarify the connections among common regularity conditions (including strong convexity, restricted secant inequality, subdifferential error bound, Polyak-{\L}ojasiewic inequality, and quadratic growth) in the class of weakly convex functions. In addition, we present a simple and modular proof for the linear convergence of the proximal point method (PPM) for convex (possibly nonsmooth) optimization using these regularity conditions. The linear convergence also holds when the subproblems of PPM are solved inexactly with a proper control of inexactness.}
}

@InProceedings{jeong128,
    title = {Parameterized Fast and Safe Tracking ({FaSTrack}) using {DeepReach}},
    author = {Jeong, Hyun Joe and Gong, Zheng and Bansal, Somil and Herbert, Sylvia},
    pages = {1006--1017},
    abstract = {Fast and Safe Tracking (FaSTrack) is a modular framework that provides safety guarantees while planning and executing trajectories in real time via value functions of Hamilton-Jacobi (HJ) reachability. These value functions are computed through dynamic programming, which is notorious for being computationally inefficient. Moreover, the resulting trajectory does not adapt online to the environment, such as sudden disturbances or obstacles. DeepReach is a scalable deep learning method to HJ reachability that allows parameterization of states, which opens up possibilities for online adaptation to various controls and disturbances. In this paper, we propose Parametric FaSTrack, which uses DeepReach to approximate a value function that parameterizes the control bounds of the planning model. The new framework can smoothly trade off between the navigation speed and the tracking error (therefore maneuverability) while guaranteeing obstacle avoidance in a priori unknown environments. We demonstrate our method through two examples and a benchmark comparison with existing methods, showing the safety, efficiency, and faster solution times of the framework.}
}

@InProceedings{lahr129,
    title = {Probabilistic {ODE} Solvers for Integration Error-Aware Numerical Optimal Control},
    author = {Lahr, Amon and Tronarp, Filip and Bosch, Nathanael and Schmidt, Jonathan and Hennig, Philipp and Zeilinger, Melanie N.},
    pages = {1018--1032},
    abstract = {Appropriate time discretization is crucial for real-time applications of numerical optimal control, such as nonlinear model predictive control. However, if the discretization error strongly depends on the applied control input, meeting accuracy and sampling time requirements simultaneously can be challenging using classical discretization methods. In particular, neither fixed-grid nor adaptive-grid discretizations may be suitable, when they suffer from large integration error or exceed the prescribed sampling time, respectively. In this work, we take a first step at closing this gap by utilizing probabilistic numerical integrators to approximate the solution of the initial value problem, as well as the computational uncertainty associated with it, inside the optimal control problem (OCP). By taking the viewpoint of probabilistic numerics and propagating the numerical uncertainty in the cost, the OCP is reformulated such that the optimal input reduces the computational uncertainty insofar as it is beneficial for the control objective. The proposed approach is illustrated using a numerical example, and potential benefits and limitations are discussed.}
}

@InProceedings{holzapfel130,
    title = {Event-Triggered Safe {B}ayesian Optimization on Quadcopters},
    author = {Holzapfel, Antonia and Brunzema, Paul and Trimpe, Sebastian},
    pages = {1033--1045},
    abstract = {Bayesian optimization (BO) has proven to be a powerful tool for automatically tuning control parameters without requiring knowledge of the underlying system dynamics. Safe BO methods, in addition, guarantee safety during the optimization process, assuming that the underlying objective function does not change. However, in real-world scenarios, time-variations frequently occur, for example, due to wear in the system or changes in operation. Utilizing standard safe BO strategies that do not address time-variations can result in failure as previous safe decisions may become unsafe over time, which we demonstrate herein. To address this, we introduce a new algorithm, Event-Triggered SafeOpt (ETSO), which adapts to changes online solely relying on the observed costs. At its core, ETSO uses an event trigger to detect significant deviations between observations and the current surrogate of the objective function. When such change is detected, the algorithm reverts to a safe backup controller, and exploration is restarted. In this way, safety is recovered and maintained across changes. We evaluate ETSO on quadcopter controller tuning, both in simulation and hardware experiments. ETSO outperforms state-of-the-art safe BO, achieving superior control performance over time while maintaining safety.}
}

@InProceedings{bai132,
    title = {Finite-Time Complexity of Incremental Policy Gradient Methods for Solving Multi-Task Reinforcement Learning},
    author = {Bai, Yitao and Doan, Thinh},
    pages = {1046--1057},
    abstract = {We consider a multi-task learning problem, where an agent is presented a number of $N$ reinforcement learning tasks. To solve this problem, we are interested in studying the gradient approach, which iteratively updates an estimate of the optimal policy using the gradients of the value functions. The classic policy gradient method, however, may be expensive to implement in the multi-task settings as it requires access to the gradients of all the tasks at every iteration. To circumvent this issue, in this paper we propose to study an incremental policy gradient method, where the agent only uses the gradient of only one task at each iteration. Our main contribution is to provide theoretical results to characterize the performance of the proposed method. In particular, we show that incremental policy gradient methods converge to the optimal value of the multi-task reinforcement learning objectives at a sublinear rate $\mathcal{O}(1/\sqrt{k})$, where $k$ is the number of iterations. To illustrate its performance, we apply the proposed method to solve a simple multi-task variant of GridWorld problems, where an agent seeks to find an policy to navigate effectively in different environments.}
}

@InProceedings{zuliani133,
    title = {Convergence guarantees for adaptive model predictive control with kinky inference},
    author = {Zuliani, Riccardo and Soloperto, Raffaele and Lygeros, John},
    pages = {1058--1070},
    abstract = {We analyze the convergence properties of a robust adaptive model predictive control algorithm used to control an unknown nonlinear system. We show that by employing a standard quadratic stabilizing cost function, and by recursively updating the nominal model through kinky inference, the resulting controller ensures convergence of the true system to the origin, despite the presence of model uncertainty. We illustrate our theoretical findings through a numerical simulation.}
}

@InProceedings{shang135,
    title = {Convex Approximations for a Bi-level Formulation of Data-Enabled Predictive Control},
    author = {Shang, Xu and Zheng, Yang},
    pages = {1071--1082},
    abstract = {The Willems' fundamental lemma, which characterizes linear time invariant (LTI) systems using input and output trajectories, has found many successful applications. Combining this with receding horizon control leads to a popular Data-EnablEd Predictive Control (DeePC) scheme. DeePC is first established for LTI systems and has been extended and applied for practical systems beyond LTI settings. However, the relationship between different DeePC variants, involving regularization and dimension reduction, remains unclear. In this paper, we first discuss a bi-level optimization formulation that combines a data pre-processing step as an inner problem (system identification) and predictive control as an outer problem (online control). We next introduce a series of convex approximations by relaxing some hard constraints in the bi-level optimization as suitable regularization terms, accounting for an implicit identification. These include some existing DeePC variants as well as two new variants, for which we establish their equivalence under appropriate settings. Notably, our analysis reveals a novel variant, called DeePC-SVD-Iter, which has remarkable empirical performance of direct methods on systems beyond deterministic LTI settings.}
}

@InProceedings{bhan138,
    title = {{PDE} Control Gym: A Benchmark for Data-Driven Boundary Control of Partial Differential Equations},
    author = {Bhan, Luke and Bian, Yuexin and Krstic, Miroslav and Shi, Yuanyuan},
    pages = {1083--1095},
    abstract = {Over the last decade, data-driven methods have surged in popularity, emerging as valuable tools for control theory. As such, neural network approximations of control feedback laws, system dynamics, and even Lyapunov functions have attracted growing attention. With the ascent of learning based control, the need for accurate, fast, and easy-to-use benchmarks has increased. In this work, we present the first learning-based environment for boundary control of PDEs. In our benchmark, we introduce three foundational PDE problems --- a 1D transport PDE, a 1D reaction-diffusion PDE, and a 2D Navier–Stokes PDE --- whose solvers are bundled in an user-friendly reinforcement learning gym. With this gym, we then present the first set of model-free, reinforcement learning algorithms for solving this series of benchmark problems, achieving stability, although at a higher cost compared to model-based PDE backstepping. With the set of benchmark environments and detailed examples, this work significantly lowers the barrier to entry for learning-based PDE control --- a topic largely unexplored by the data-driven control community. The entire benchmark is available on Github along with detailed documentation and the presented reinforcement learning models are open sourced.}
}

@InProceedings{wang140,
    title = {Towards Bio-Inspired Control of Aerial Vehicle: Distributed Aerodynamic Parameters for State Prediction},
    author = {Wang, Yikang and Perrusquia, Adolfo and Ignatyev, Dmitry},
    pages = {1096--1106},
    abstract = {In an era where traditional flight control systems are increasingly strained by the demands of modern aerial missions, this research introduces a novel integration of bio-inspired sensing mechanisms into aerial vehicle control systems, aimed at revolutionizing the adaptability and efficiency of UAV operations. Current gust suppression technologies often activate only after disturbances have occurred, highlighting significant limitations in real-time responsiveness and computational efficiency. With a specific emphasis on employing distributed aerodynamic parameters for predicting flight states, the study utilizes a Convolutional Neural Network (CNN) and a Long Short-Term Memory (LSTM) network to investigate the predictive capabilities of these models under varying conditions, including scenarios with full and limited input data. The models were assessed on their ability to forecast the pitch rate of Unmanned Aerial Vehicles (UAVs), examining both the precision of predictions in response to different historical input sizes and their robustness against simulated sensor noise. Results highlight the potential of using aerodynamic data to enhance the reliability and adaptability of flight control systems, significantly reducing dependency on specific sensor inputs. This approach not only demonstrates the effectiveness of integrating sophisticated machine learning models with aerospace technology but also paves the way for more adaptive, efficient control systems in UAV operations.}
}

@InProceedings{nakhaeinezhadfard141,
    title = {Residual Learning and Context Encoding for Adaptive Offline-to-Online Reinforcement Learning},
    author = {Nakhaeinezhadfard, Mohammadreza and Scannell, Aidan and Pajarinen, Joni},
    pages = {1107--1121},
    abstract = {Offline reinforcement learning (RL) allows learning sequential behavior from fixed datasets. Since offline datasets do not cover all possible situations, many methods collect additional data during online fine-tuning to improve performance. In general, these methods assume that the transition dynamics remain the same during both the offline and online phases of training. However, in many real-world applications, such as outdoor construction and navigation over rough terrain, it is common for the transition dynamics to vary between the offline and online phases. Moreover, the dynamics may vary during the online training. To address this problem of changing dynamics from offline to online RL we propose a residual learning approach that infers dynamics changes to correct the outputs of the offline solution. At the online fine-tuning phase, we train a context encoder to learn a representation that is consistent inside the current online learning environment while being able to predict dynamic transitions. Experiments in D4RL MuJoCo environments, modified to support dynamics' changes upon environment resets, show that our approach can adapt to these dynamic changes and generalize to unseen perturbations in a sample-efficient way, whilst comparison methods cannot.}
}

@InProceedings{yi142,
    title = {CoVO-MPC: Theoretical Analysis of Sampling-based {MPC} and Optimal Covariance Design},
    author = {Yi, Zeji and Pan, Chaoyi and He, Guanqi and Qu, Guannan and Shi, Guanya},
    pages = {1122--1135},
    abstract = {Sampling-based Model Predictive Control (MPC) has been a practical and effective approach in many domains, notably model-based reinforcement learning, thanks to its flexibility and parallelizability. Despite its appealing empirical performance, the theoretical understanding, particularly in terms of convergence analysis and hyperparameter tuning, remains absent. In this paper, we characterize the convergence property of a widely used sampling-based MPC method, Model Predictive Path Integral Control (MPPI). We show that MPPI enjoys at least linear convergence rates when the optimization is quadratic, which covers time-varying LQR systems. We then extend to more general nonlinear systems. Our theoretical analysis directly leads to a novel sampling-based MPC algorithm, CoVariance-Optimal MPC (CoVO-MPC) that optimally schedules the sampling covariance to optimize the convergence rate. Empirically, CoVO-MPC significantly outperforms standard MPPI by 43-54\% in both simulations and real-world quadrotor agile control tasks. Videos and Appendices are available at https://tinyurl.com/covo-mpc-cmu.}
}

@InProceedings{song143,
    title = {Stable Modular Control via Contraction Theory for Reinforcement Learning},
    author = {Song, Bing and Slotine, Jean-Jacques and Pham, Quang-Cuong},
    pages = {1136--1148},
    abstract = {We propose a novel way to integrate control theoretical results with reinforcement learning (RL) for stability, robustness, and generalization: developing modular control architectures via contraction theory to simplify the complex problems. To guarantee control stability for RL, we leverage modularity to deconstruct the nonlinear stability problems into algebraically solvable ones, yielding linear constraints on the input gradients of control networks that can be as simple as switching the signs of network weights. This control architecture can be implemented in general RL frameworks without modifying the algorithms. This minimally invasive way allows arguably easy integration into hierarchical RL, and improves its performance. We realize the modularity by constructing an auxiliary space through coordinate transformation. Within the auxiliary space, system dynamics can be represented as hierarchical combinations of subsystems. These subsystems converge recursively following their hierarchies, provided stable self-feedbacks. We implement this modular control architecture in PPO and hierarchical RL, and demonstrate in simulation (i) the necessity of control stability for robustness and generalization and (ii) the effectiveness in improving hierarchical RL for manipulation learning.}
}

@InProceedings{tang144,
    title = {Data-Driven Bifurcation Analysis via Learning of Homeomorphism},
    author = {Tang, Wentao},
    pages = {1149--1160},
    abstract = {This work proposes a data-driven approach for bifurcation analysis in nonlinear systems when the governing differential equations are not available. Specifically, regularized regression with barrier terms is used to learn a homeomorphism that transforms the underlying system to a reference linear dynamics --- either an explicit reference model with desired qualitative behavior, or Koopman eigenfunctions that are identified from some system data under a reference parameter value. When such a homeomorphism fails to be constructed with low error, bifurcation phenomenon is detected. A case study is performed on a planar numerical example where a pitchfork bifurcation exists.}
}

@InProceedings{fey146,
    title = {A Learning-based Framework to Adapt Legged Robots On-the-fly to Unexpected Disturbances},
    author = {Fey, Nolan and Li, He and Adrian, Nicholas and Wensing, Patrick and Lemmon, Michael},
    pages = {1161--1173},
    abstract = {State-of-the-art control methods for legged robots demonstrate impressive performance and robustness on a variety of terrains. Still, these approaches often lack an ability to learn how to adapt to changing conditions online. Such adaptation is especially critical if the robot encounters an environment with dynamics different than those considered in its model or in prior offline training. This paper proposes a learning-based framework that allows a walking robot to stabilize itself under disturbances neglected by its base controller. We consider an approach that simplifies the learning problem into two tasks: learning a model to estimate the robot's steady-state response and learning a dynamics model for the system near its steady-state behavior. Through experiments with the MIT Mini Cheetah, we show that we can learn these models offline in simulation and transfer them to the real world, optionally finetuning them as the robot collects data. We demonstrate the effectiveness of our approach by applying it to stabilize the quadruped as it carries a box of water on its back.}
}

@InProceedings{shin149,
    title = {On Task-Relevant Loss Functions in Meta-Reinforcement Learning},
    author = {Shin, Jaeuk and Kim, Giho and Lee, Howon and Han, Joonho and Yang, Insoon},
    pages = {1174--1186},
    abstract = {Designing a competent meta-reinforcement learning (meta-RL) algorithm in terms of data usage remains a central challenge to be tackled for its successful real-world applications. In this paper, we propose a sample-efficient meta-RL algorithm that learns a model of the system or environment at hand in a task-directed manner. As opposed to the standard model-based approaches to meta-RL, our method exploits the value information in order to rapidly capture the decision-critical part of the environment. The key component of our method is the loss function for learning both the task inference module and the system model. This systematically couples the model discrepancy and the value estimate, thereby enabling our proposed algorithm to learn the policy and task inference module with a significantly smaller amount of data compared to the existing meta-RL algorithms. The proposed method is evaluated in high-dimensional robotic control, empirically verifying its effectiveness in extracting information indispensable for solving the tasks from observations in a sample-efficient manner.}
}

@InProceedings{zhan150,
    title = {State-Wise Safe Reinforcement Learning with Pixel Observations},
    author = {Zhan, Sinong and Wang, Yixuan and Wu, Qingyuan and Jiao, Ruochen and Huang, Chao and Zhu, Qi},
    pages = {1187--1201},
    abstract = {In the context of safe exploration, Reinforcement Learning (RL) has long grappled with the challenges of balancing the tradeoff between maximizing rewards and minimizing safety violations, particularly in complex environments with contact-rich or non-smooth dynamics, and when dealing with high-dimensional pixel observations. Furthermore, incorporating state-wise safety constraints in the exploration and learning process, where the agent must avoid unsafe regions without prior knowledge, adds another layer of complexity. In this paper, we propose a novel pixel-observation safe RL algorithm that efficiently encodes state-wise safety constraints with unknown hazard regions through a newly introduced latent barrier-like function learning mechanism. As a joint learning framework, our approach begins by constructing a latent dynamics model with low-dimensional latent spaces derived from pixel observations. We then build and learn a latent barrier-like function on top of the latent dynamics and conduct policy optimization simultaneously, thereby improving both safety and the total expected return. Experimental evaluations on the safety-gym benchmark suite demonstrate that our proposed method significantly reduces safety violations throughout the training process, and demonstrates faster safety convergence compared to existing methods while achieving competitive results in reward return.}
}

@InProceedings{agorio152,
    title = {Multi-agent assignment via state augmented reinforcement learning},
    author = {Agorio, Leopoldo and Alen, Sean Van and Calvo-Fullana, Miguel and Paternain, Santiago and Bazerque, Juan Andr\'{e}s},
    pages = {1202--1213},
    abstract = {We address the conflicting requirements of a multi-agent assignment problem through constrained reinforcement learning, emphasizing the inadequacy of standard regularization techniques for this purpose. Instead, we recur to a state augmentation approach in which the oscillation of dual variables is exploited by agents to alternate between tasks. In addition, we coordinate the actions of the multiple agents acting on their local states through these multipliers, which are gossiped through a communication network, eliminating the need to access other agent states. By these means, we propose a distributed multi-agent assignment protocol with theoretical feasibility guarantees that we corroborate in a monitoring numerical experiment.}
}

@InProceedings{hoffmann156,
    title = {{PlanNetX}: Learning an Efficient Neural Network Planner from {MPC} for Longitudinal Control},
    author = {Hoffmann, Jasper and Clausen, Diego Fernandez and Brosseit, Julien and Bernhard, Julian and Esterle, Klemens and Werling, Moritz and Karg, Michael and B\"{o}decker, Joschka Joschka},
    pages = {1214--1227},
    abstract = {Model predictive control (MPC) is a powerful, optimization-based approach for controlling dynamical systems. However, the computational complexity of online optimization can be problematic on embedded devices. Especially, when we need to guarantee fixed control frequencies. Thus, previous work proposed to reduce the computational burden using imitation learning (IL) approximating the MPC policy by a neural network. In this work, we instead learn the whole planned trajectory of the MPC. We introduce a combination of a novel neural network architecture PlanNetX and a simple loss function based on the state trajectory that leverages the parameterized optimal control structure of the MPC. We validate our approach in the context of autonomous driving by learning a longitudinal planner and benchmarking it extensively in the CommonRoad simulator using synthetic scenarios and scenarios derived from real data. Our experimental results show that we can learn the open-loop MPC trajectory with high accuracy while improving the closed-loop performance of the learned control policy over other baselines like behavior cloning.}
}

@InProceedings{drummond157,
    title = {Mapping back and forth between model predictive control and neural networks},
    author = {Drummond, Ross and Baldivieso, Pablo and Valmorbida, Giorgio},
    pages = {1228--1240},
    abstract = {Model predictive control (MPC) for linear systems with quadratic costs and linear constraints is shown to admit an exact representation as an implicit neural network. A method to ``unravel'' the implicit neural network of MPC into an explicit one is also introduced. As well as building links between model-based and data-driven control, these results emphasize the capability of implicit neural networks for representing solutions of optimisation problems, as such problems are themselves implicitly defined functions.}
}

@InProceedings{raghavan159,
    title = {A Multi-Modal Distributed Learning Algorithm in Reproducing Kernel {H}ilbert Spaces},
    author = {Raghavan, Aneesh and Johansson, Karl Henrik},
    pages = {1241--1252},
    abstract = {We consider the problem of function estimation by a multi-agent system consisting of two agents and a fusion center. Each agent receives data comprising of samples of an independent variable (input) and the corresponding values of the dependent variable (output). The data remains local and is not shared with other members in the system. The objective of the system is to collaboratively estimate the function from the input to the output. To this end, we present an iterative distributed algorithm for this function estimation problem. Each agent solves a local estimation problem in a Reproducing Kernel Hilbert Space (RKHS) and uploads the function to the fusion center. At the fusion center, the functions are fused by first estimating the data points that would have generated the uploaded functions and then subsequently solving a least squares estimation problem using the estimated data from both functions. The fused function is downloaded by the agents and is subsequently used for estimation at the next iteration along with incoming data. This procedure is executed sequentially and stopped when the difference between consecutively estimated functions becomes small enough. With respect to the algorithm, we prove existence of basis functions for suitable representation of estimated functions and present closed form solutions to the estimation problems at the agents and the fusion center.}
}

@InProceedings{mitra162,
    title = {Towards Model-Free {LQR} Control over Rate-Limited Channels},
    author = {Mitra, Aritra and Ye, Lintao and Gupta, Vijay},
    pages = {1253--1265},
    abstract = {Given the success of model-free methods for control design in many problem settings, it is natural to ask how things will change if realistic communication channels are utilized for the transmission of gradients or policies. While the resulting problem has analogies with the formulations studied under the rubric of networked control systems, the rich literature in that area has typically assumed that the model of the system is known. As a step towards bridging the fields of model-free control design and networked control systems, we ask: \textit{Is it possible to solve basic control problems - such as the linear quadratic regulator (LQR) problem - in a model-free manner over a rate-limited channel?} Toward answering this question, we study a setting where a worker agent transmits quantized policy gradients (of the LQR cost) to a server over a noiseless channel with a finite bit-rate. We propose a new algorithm titled Adaptively Quantized Gradient Descent (\texttt{AQGD}), and prove that above a certain finite threshold bit-rate, \texttt{AQGD} guarantees exponentially fast convergence to the globally optimal policy, with \textit{no deterioration of the exponent relative to the unquantized setting}. More generally, our approach reveals the benefits of adaptive quantization in preserving fast linear convergence rates, and, as such, may be of independent interest to the literature on compressed optimization.}
}

@InProceedings{shehab163,
    title = {Learning True Objectives: Linear Algebraic Characterizations of Identifiability in Inverse Reinforcement Learning},
    author = {Shehab, Mohamad Louai and Aspeel, Antoine and Arechiga, Nikos and Best, Andrew and Ozay, Necmiye},
    pages = {1266--1277},
    abstract = {Inverse reinforcement Learning (IRL) has emerged as a powerful paradigm for extracting expert skills from observed behavior, with applications ranging from autonomous systems to humanrobot interaction. However, the identifiability issue within IRL poses a significant challenge, as multiple reward functions can explain the same observed behavior. This paper provides a linear algebraic characterization of several identifiability notions for an entropy-regularized finite horizon Markov decision process (MDP). Moreover, our approach allows for the seamless integration of prior knowledge, in the form of featurized reward functions, to enhance the identifiability of IRL problems. The results are demonstrated with experiments on a grid world environment.}
}

@InProceedings{lavanakul164,
    title = {Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes},
    author = {Lavanakul, Will and Choi, Jason and Sreenath, Koushil and Tomlin, Claire},
    pages = {1278--1291},
    abstract = {Learning-based approaches are emerging as an effective approach for safety filters for black-box dynamical systems. Existing methods have relied on certificate functions like Control Barrier Functions (CBFs) and Hamilton-Jacobi (HJ) reachability value functions. The primary motivation for our work is the recognition that ultimately, enforcing the safety constraint as a control input constraint at each state is what matters. By focusing on this constraint, we can eliminate dependence on any specific certificate function-based design. To achieve this, we define a discriminating hyperplane that shapes the half-space constraint on control input at each state, serving as a sufficient condition for safety. This concept not only generalizes over traditional safety methods but also simplifies safety filter design by eliminating dependence on specific certificate functions. We present two strategies to learn the discriminating hyperplane: (a) a supervised learning approach, using pre-verified control invariant sets for labeling, and (b) a reinforcement learning (RL) approach, which does not require such labels. The main advantage of our method, unlike conventional safe RL approaches, is the separation of performance and safety. This offers a reusable safety filter for learning new tasks, avoiding the need to retrain from scratch. As such, we believe that the new notion of the discriminating hyperplane offers a more generalizable direction towards designing safety filters, encompassing and extending existing certificate-function-based or safe RL methodologies.}
}

@InProceedings{giacomuzzo166,
    title = {Lagrangian Inspired Polynomial Estimator for black-box learning and control of underactuated systems},
    author = {Giacomuzzo, Giulio and Cescon, Riccardo and Romeres, Diego and Carli, Ruggero and Libera, Alberto Dalla},
    pages = {1292--1304},
    abstract = {The Lagrangian Inspired Polynomial (LIP) estimator (Giacomuzzo et al., 2023) is a black-box estimator based on Gaussian Process Regression, recently presented for the inverse dynamics identi- fication of Lagrangian systems. It relies on a novel multi-output kernel that embeds the structure of the Euler-Lagrange equation. In this work, we extend its analysis to the class of underactuated robots. First, we show that, despite being a black-box model, the LIP allows estimating kinetic and potential energies, as well as the inertial, Coriolis, and gravity components directly from the overall torque measures. Then we exploit these properties to derive a two-stage energy-based controller for the swing-up and stabilization of balancing robots. Experimental results on a simulated Pendubot confirm the feasibility of the proposed approach.}
}

@InProceedings{bajelani169,
    title = {From Raw Data to Safety: Reducing Conservatism by Set Expansion},
    author = {Bajelani, Mohammad and Heusden, Klaske Van},
    pages = {1305--1317},
    abstract = {In response to safety concerns associated with learning-based algorithms, safety filters have been proposed as a modular technique. Generally, these filters heavily rely on the system's model, which is contradictory if they are intended to enhance a data-driven or end-to-end learning solution. This paper extends our previous work, a purely Data-Driven Safety Filter (DDSF) based on Willems' lemma, to an extremely short-sighted and non-conservative solution. Specifically, we propose online and offline sample-based methods to expand the safe set of DDSF and reduce its conservatism. Since this method is defined in an input-output framework, it can systematically handle both unknown and time-delay LTI systems using only one single batch of data. To evaluate its performance, we apply the proposed method to a time-delay system under various settings. The simulation results validate the effectiveness of the set expansion algorithm in generating a notably large input-output safe set, resulting in safety filters that are not conservative, even with an extremely short prediction horizon.}
}

@InProceedings{ordonez-apraez170,
    title = {Dynamics Harmonic Analysis of Robotic Systems: Application in Data-Driven {K}oopman Modelling},
    author = {Ordo\~{n}ez-Apraez, Daniel and Kostic, Vladimir and Turrisi, Giulio and Novelli, Pietro and Mastalli, Carlos and Semini, Claudio and Pontil, Massimilano},
    pages = {1318--1329},
    abstract = {We introduce the use of harmonic analysis to decompose the state space of symmetric robotic systems into orthogonal isotypic subspaces. These are lower-dimensional spaces that capture distinct, symmetric, and synergistic motions. For linear dynamics, we characterize how this decomposition leads to a subdivision of the dynamics into independent linear systems on each subspace, a property we term dynamics harmonic analysis (DHA). To exploit this property, we use Koopman operator theory to propose an equivariant deep-learning architecture that leverages the properties of DHA to learn a global linear model of the system dynamics. Our architecture, validated on synthetic systems and the dynamics of locomotion of a quadrupedal robot, exhibits enhanced generalization, sample efficiency, and interpretability, with less trainable parameters and computational costs.}
}

@InProceedings{stamouli172,
    title = {Recursively Feasible Shrinking-Horizon {MPC} in Dynamic Environments with Conformal Prediction Guarantees},
    author = {Stamouli, Charis and Lindemann, Lars and Pappas, George},
    pages = {1330--1342},
    abstract = {In this paper, we focus on the problem of shrinking-horizon Model Predictive Control (MPC) in uncertain dynamic environments. We consider controlling a deterministic autonomous system that interacts with uncontrollable stochastic agents during its mission. Employing tools from conformal prediction, existing works derive high-confidence prediction regions for the unknown agent trajectories, and integrate these regions in the design of suitable safety constraints for MPC. Despite guaranteeing probabilistic safety of the closed-loop trajectories, these constraints do not ensure feasibility of the respective MPC schemes for the entire duration of the mission. We propose a shrinking-horizon MPC that guarantees recursive feasibility via a gradual relaxation of the safety constraints as new prediction regions become available online. This relaxation enforces the safety constraints to hold over the least restrictive prediction region from the set of all available prediction regions. In a comparative case study with the state of the art, we empirically show that our approach results in tighter prediction regions and verify recursive feasibility of our MPC scheme.}
}

@InProceedings{tumu173,
    title = {Multi-Modal Conformal Prediction Regions by Optimizing Convex Shape Templates},
    author = {Tumu, Renukanandan and Cleaveland, Matthew and Mangharam, Rahul and Pappas, George and Lindemann, Lars},
    pages = {1343--1356},
    abstract = {Conformal prediction is a statistical tool for producing prediction regions for machine learning models that are valid with high probability. A key component of conformal prediction algorithms is a non-conformity score function that quantifies how different a model's prediction is from the unknown ground truth value. Essentially, these functions determine the shape and the size of the conformal prediction regions. However, little work has gone into finding non-conformity score functions that produce prediction regions that are multi-modal and practical, i.e., that can efficiently be used in engineering applications. We propose a method that optimizes parameterized shape template functions over calibration data, which results in non-conformity score functions that produce prediction regions with minimum volume. Our approach results in prediction regions that are multi-modal, so they can properly capture residuals of distributions that have multiple modes, and practical, so each region is convex and can be easily incorporated into downstream tasks, such as a motion planner using conformal prediction regions. Our method applies to general supervised learning tasks, while we illustrate its use in time-series prediction. We provide a toolbox and present illustrative case studies of F16 fighter jets and autonomous vehicles, showing an up to 68\% reduction in prediction region area.}
}

@InProceedings{kang174,
    title = {Learning Locally Interacting Discrete Dynamical Systems: Towards Data-Efficient and Scalable Prediction},
    author = {Kang, Beomseok and Kumar, Harshit and Lee, Minah and Chakraborty, Biswadeep and Mukhopadhyay, Saibal},
    pages = {1357--1369},
    abstract = {Locally interacting dynamical systems, such as epidemic spread, rumor propagation through crowd, and forest fire, exhibit complex global dynamics originated from local, relatively simple, and often stochastic interactions between dynamic elements. Their temporal evolution is often driven by transitions between a finite number of discrete states. Despite significant advancements in predictive modeling through deep learning, such interactions among many elements have rarely explored as a specific domain for predictive modeling. We present Attentive Recurrent Neural Cellular Automata (AR-NCA), to effectively discover unknown local state transition rules by associating the temporal information between neighboring cells in a permutation-invariant manner. AR-NCA exhibits the superior generalizability across various system configurations (i.e., spatial distribution of states), data efficiency and robustness in extremely data-limited scenarios even in the presence of stochastic interactions, and scalability through spatial dimension-independent prediction.}
}

@InProceedings{mao176,
    title = {How Safe Am I Given What I See? {C}alibrated Prediction of Safety Chances for Image-Controlled Autonomy},
    author = {Mao, Zhenjiang and Sobolewski, Carson and Ruchkin, Ivan},
    pages = {1370--1387},
    abstract = {End-to-end learning has emerged as a major paradigm for developing autonomous controllers. Unfortunately, with its performance and convenience comes an even greater challenge of safety assurance. A key factor in this challenge is the absence of low-dimensional and interpretable dynamical states, around which traditional assurance methods revolve. Focusing on the online safety prediction problem, this paper systematically investigates a flexible family of learning pipelines based on generative world models, which do not require low-dimensional states. To implement these pipelines, we overcome the challenges of missing safety labels under prediction-induced distribution shift and learning safety-informed latent representations. Moreover, we provide statistical calibration guarantees for our safety chance predictions based on conformal inference. An extensive evaluation of our predictor family on two image-controlled case studies, a racing car and a cartpole, delivers counterintuitive results and highlights open problems in deep safety prediction.}
}

@InProceedings{drummond177,
    title = {Convex neural network synthesis for robustness in the 1-norm},
    author = {Drummond, Ross and Guiver, Chris and Turner, Matthew},
    pages = {1388--1399},
    abstract = {With neural networks being used to control safety-critical systems, they increasingly have to be both accurate (in the sense of matching inputs to outputs) and robust. However, these two properties are often at odds with each other and a trade-off has to be navigated. To address this issue, this paper proposes a method to generate an approximation of a neural network which is certifiably more robust. Crucially, the method is fully convex and posed as a semi-definite programme. An application to robustifying model predictive control is used to demonstrate the results. The aim of this work is to introduce a method to navigate the neural network robustness/accuracy trade-off.}
}

@InProceedings{hosseinkhanboucher179,
    title = {Increasing Information for Model Predictive Control with Semi-{M}arkov Decision Processes},
    author = {Hosseinkhan Boucher, R\'{e}my and Douka, Stella and Semeraro, Onofrio and Mathelin, Lionel},
    pages = {1400--1414},
    abstract = {Recent works in Learning-Based Model Predictive Control of dynamical systems show impressive sample complexity performances using criteria from Information Theory to accelerate the learning procedure. However, the sequential exploration opportunities are limited by the system local state, restraining the amount of information of the observations from the current exploration trajectory. This article resolves this limitation by introducing temporal abstraction through the framework of Semi-Markov Decision Processes. The framework increases the total information of the gathered data for a fixed sampling budget, thus reducing the sample complexity.}
}

@InProceedings{dai180,
    title = {Physically Consistent Modeling \& Identification of Nonlinear Friction with Dissipative Gaussian Processes},
    author = {Dai, Rui and Evangelisti, Giulio and Hirche, Sandra},
    pages = {1415--1426},
    abstract = {Friction modeling has always been a challenging problem due to the complexity of real physical systems. Although a few state-of-the-art structured data-driven methods show their efficiency in nonlinear system modeling, deterministic passivity as one of the significant characteristics of friction is rarely considered in these methods. To address this issue, we propose a Gaussian Process based model that preserves the inherent structural properties such as passivity. A matrix-vector physical structure is considered in our approaches to ensure physical consistency, in particular, enabling a guarantee of positive semi-definiteness of the damping matrix. An aircraft benchmark simulation is employed to demonstrate the efficacy of our methodology. Estimation accuracy and data efficiency are increased substantially by considering and enforcing more structured physical knowledge. Also, the fulfillment of the dissipative nature of the aerodynamics is validated numerically.}
}

@InProceedings{kumawat181,
    title = {{STEMFold}: Stochastic Temporal Manifold for Multi-Agent Interactions in the Presence of Hidden Agents},
    author = {Kumawat, Hemant and Chakraborty, Biswadeep and Mukhopadhyay, Saibal},
    pages = {1427--1439},
    abstract = {Learning accurate, data-driven predictive models for multiple interacting agents following unknown dynamics is crucial in many real-world physical and social systems. In many scenarios, dynamics prediction must be performed under incomplete observations, i.e., only a subset of agents are known and observable from a larger topological system while the behaviors of the unobserved agents and their interactions with the observed agents are not known. When only incomplete observations of a dynamical system are available, so that some states remain hidden, it is generally not possible to learn a closed-form model in these variables using either analytic or data-driven techniques. In this work, we propose STEMFold, a spatiotemporal attention-based generative model, to learn a stochastic manifold to predict the underlying unmeasured dynamics of the multi-agent system from observations of only visible agents. Our analytical results motivate STEMFold design using a spatiotemporal graph with time anchors to effectively map the observations of visible agents to a stochastic manifold with no prior information about interaction graph topology. We empirically evaluated our method on two simulations and two real-world datasets, where it outperformed existing networks in predicting complex multiagent interactions, even with many unobserved agents.}
}

@InProceedings{meshkatalsadat182,
    title = {Distributed On-the-Fly Control of Multi-Agent Systems With Unknown Dynamics: Using Limited Data to Obtain Near-optimal Control},
    author = {Meshkat Alsadat, Shayan and Baharisangari, Nasim and Xu, Zhe},
    pages = {1440--1451},
    abstract = {We propose a method called ODMU for ``on-the-fly control of distributed multi-agent systems with unknown nonlinear dynamics'' and with (a)synchronous communication between the agents where data from a single finite-horizon trajectory is used, possibly in conjunction with side information. ODMU can be applied to real-time scenarios when the dynamics of the system are unknown or suddenly change such that a priori known model cannot be applied. In our proposed algorithm, the agents communicate their states using (a)synchronous communication and exploit the side information, e.g., regularities of the system, states, agents' communication scheme, algebraic limitations, and coupling in the system states. We provide ODMU for over-approximating the reachable sets and to control the agents under conditions with severely limited data. ODMU creates differential inclusion sets that calculate the over approximations of the reachable sets containing the unknown vector field. We show that ODMU calculates the near-optimal control and calculates an upper bound (suboptimality bound) for the error between the optimal trajectory and the trajectory calculated by ODMU. We use convex-optimization-based control to obtain the guaranteed near-optimal solution. We demonstrate the effect of side information on obtaining smaller bounds on suboptimality by applying ODMU on a system of unicycles. Additionally, we present a case study where a multi-agent system of unicycles with unknown dynamics is controlled via ODMU. Moreover, we have developed two baselines, SINDYcMulti and CGP-LCBMulti to compare our method with them.}
}

@InProceedings{alboni183,
    title = {CACTO-SL: Using {S}obolev Learning to improve Continuous Actor-Critic with Trajectory Optimization},
    author = {Alboni, Elisa and Grandesso, Gianluigi and Rosati Papini, Gastone Pietro and Carpentier, Justin and Del Prete, Andrea},
    pages = {1452--1463},
    abstract = {Trajectory Optimization (TO) and Reinforcement Learning (RL) are powerful and complementary tools to solve optimal control problems. On the one hand, TO can efficiently compute locally-optimal solutions, but it tends to get stuck in local minima if the problem is not convex. On the other hand, RL is typically less sensitive to non-convexity, but it requires a much higher computational effort. Recently, we have proposed CACTO (Continuous Actor-Critic with Trajectory Optimization), an algorithm that uses TO to guide the exploration of an actor-critic RL algorithm. In turns, the policy encoded by the actor is used to warm-start TO, closing the loop between TO and RL. In this work, we present an extension of CACTO exploiting the idea of Sobolev learning. To make the training of the critic network faster and more data efficient, we enrich it with the gradient of the Value function, computed via a backward pass of the differential dynamic programming algorithm. Our results show that the new algorithm is more efficient than the original CACTO, reducing the number of TO episodes by a factor ranging from 3 to 10, and consequently the computation time. Moreover, we show that CACTO-SL helps TO to find better minima and to produce more consistent results.}
}

@InProceedings{zhang184,
    title = {Multi-Agent Coverage Control with Transient Behavior Consideration},
    author = {Zhang, Runyu and Ma, Haitong and Li, Na},
    pages = {1464--1476},
    abstract = {This paper studies the multi-agent coverage control (MAC) problem where agents must dynamically learn an unknown density function while performing coverage tasks. Unlike many current theoretical frameworks that concentrate solely on the regret occurring at specific targeted sensory locations, our approach additionally considers the regret caused by transient behavior – the path from one location and another. We propose the multi-agent coverage control with the doubling trick (MAC-DT) algorithm and demonstrate that it achieves (approximated) regret of $\widetilde O(\sqrt{T})$ even when accounting for the transient behavior. Our result is also supported by numerical experiments, showcasing that the proposed algorithm manages to match or even outperform the baseline algorithms in simulation environments. We also show how our algorithm can be modified to handle safety constraints and further implement the algorithm on a real-robotic testbed.}
}

@InProceedings{strong185,
    title = {Data Driven Verification of Positive Invariant Sets for Discrete, Nonlinear Systems},
    author = {Strong, Amy K. and Bridgeman, Leila J.},
    pages = {1477--1488},
    abstract = {Invariant sets are essential for understanding the stability and safety of nonlinear systems. However, certifying the existence of a positive invariant set for a nonlinear model is difficult and often requires knowledge of the system's dynamic model. This paper presents a data driven method to certify a positive invariant set for an unknown, discrete, nonlinear system. A triangulation of a subset of the state space is used to query data points. Then, linear programming is used to create a continuous piecewise affine function that fulfills the criteria of the Extended Invariant Set Principle by leveraging an inequality error bound that uses the Lipschitz constant of the unknown system. Numerical results demonstrate the program's ability to certify positive invariant sets from sampled data.}
}

@InProceedings{clark186,
    title = {Adaptive Teaching in Heterogeneous Agents: Balancing Surprise in Sparse Reward Scenarios},
    author = {Clark, Emma and Ryu, Kanghyun and Mehr, Negar},
    pages = {1489--1501},
    abstract = {Learning from Demonstration (LfD) can be an efficient way to train systems with analogous agents by enabling ``Student'' agents to learn from the demonstrations of the most experienced ``Teacher'' agent, instead of training their policy in parallel. However, when there are discrepancies in agent capabilities, such as divergent actuator power or joint angle constraints, naively replicating demonstrations that are out of bounds for the Student's capability can limit efficient learning. We present a Teacher-Student learning framework specifically tailored to address the challenge of heterogeneity between the Teacher and Student agents. Our framework is based on the concept of ``surprise'', inspired by its application in exploration incentivization in sparse-reward environments. Surprise is repurposed to enable the Teacher to detect and adapt to differences between itself and the Student. By focusing on maximizing its surprise in response to the environment while concurrently minimizing the Student's surprise in response to the demonstrations, the Teacher agent can effectively tailor its demonstrations to the Student's specific capabilities and constraints. We validate our method by demonstrating improvements in the Student's learning in control tasks within sparse-reward environments.}
}

@InProceedings{goel187,
    title = {Can a Transformer Represent a {K}alman Filter?},
    author = {Goel, Gautam and Bartlett, Peter},
    pages = {1502--1512},
    abstract = {Transformers are a class of autoregressive deep learning architectures which have recently achieved state-of-the-art performance in various vision, language, and robotics tasks. We revisit the problem of Kalman Filtering in linear dynamical systems and show that Transformers can approximate the Kalman Filter in a strong sense. Specifically, for any observable LTI system we construct an explicit causally-masked Transformer which implements the Kalman Filter, up to a small additive error which is bounded uniformly in time; we call our construction the Transformer Filter. Our construction is based on a two-step reduction. We first show that a softmax self-attention block can exactly represent a Nadaraya–Watson kernel smoothing estimator with a Gaussian kernel. We then show that this estimator closely approximates the Kalman Filter. We also investigate how the Transformer Filter can be used for measurement-feedback control and prove that the resulting nonlinear controllers closely approximate the performance of standard optimal control policies such as the LQG controller.}
}

@InProceedings{sun188,
    title = {Data-Driven Simulator for Mechanical Circulatory Support with Domain Adversarial Neural Process},
    author = {Sun, Sophia and Chen, Wenyuan and Zhou, Zihao and Fereidooni, Sonia and Jortberg, Elise and Yu, Rose},
    pages = {1513--1525},
    abstract = {We propose a data-driven simulator for Mechanical Circulatory Support (MCS) devices, implemented as a probabilistic deep sequence model. Existing mechanical simulators for MCS rely on oversimplifying assumptions and are insensitive to patient-specific behavior, limiting their applicability to real-world treatment scenarios. To address these shortcomings, our model Domain Adversarial Neural Process (DANP) employs a neural process architecture, allowing it to capture the probabilistic relationship between MCS pump levels and aortic pressure measurements with uncertainty. We use domain adversarial training to combine real-world and simulation data, resulting in a more realistic and diverse representation of potential outcomes. Empirical results with an improvement of 19\% in non-stationary trend prediction establish DANP as an effective tool for clinicians to understand and make informed decisions regarding MCS patient treatment.}
}

@InProceedings{lin189,
    title = {{DC4L}: Distribution Shift Recovery via Data-Driven Control for Deep Learning Models},
    author = {Lin, Vivian and Jang, Kuk Jin and Dutta, Souradeep and Caprio, Michele and Sokolsky, Oleg and Lee, Insup},
    pages = {1526--1538},
    abstract = {Deep neural networks have repeatedly been shown to be non-robust to the uncertainties of the real world, even to naturally occurring ones. A vast majority of current approaches have focused on data-augmentation methods to expand the range of perturbations that the classifier is exposed to while training. A relatively unexplored avenue that is equally promising involves sanitizing an image as a preprocessing step, depending on the nature of perturbation. In this paper, we propose to use control for learned models to recover from distribution shifts online. Specifically, our method applies a sequence of semantic-preserving transformations to bring the shifted data closer in distribution to the training set, as measured by the Wasserstein distance. Our approach is to 1) formulate the problem of distribution shift recovery as a Markov decision process, which we solve using reinforcement learning, 2) identify a minimum condition on the data for our method to be applied, which we check online using a binary classifier, and 3) employ dimensionality reduction through orthonormal projection to aid in our estimates of the Wasserstein distance. We provide theoretical evidence that orthonormal projection preserves characteristics of the data at the distributional level. We apply our distribution shift recovery approach to the ImageNet-C benchmark for distribution shifts, demonstrating an improvement in average accuracy of up to 14.21\% across a variety of state-of-the-art ImageNet classifiers. We further show that our method generalizes to composites of shifts from the ImageNet-C benchmark, achieving improvements in average accuracy of up to 9.81\%. Finally, we test our method on CIFAR-100-C and report improvements of up to 8.25\%.}
}

@InProceedings{zeng190,
    title = {QCQP-Net: Reliably Learning Feasible Alternating Current Optimal Power Flow Solutions Under Constraints},
    author = {Zeng, Sihan and Kim, Youngdae and Ren, Yuxuan and Kim, Kibaek},
    pages = {1539--1551},
    abstract = {At the heart of power system operations, alternating current optimal power flow (ACOPF) studies the generation of electric power in the most economical way under network-wide load requirement, and can be formulated as a highly structured non-convex quadratically constrained quadratic program (QCQP). Optimization-based solutions to ACOPF (such as ADMM or interior-point method), as the classic approach, require large amount of computation and cannot meet the need to repeatedly solve the problem as load requirement frequently changes. On the other hand, learning-based methods that directly predict the ACOPF solution given the load input incur little computational cost but often generates infeasible solutions (i.e. violate the constraints of ACOPF). In this work, we combine the best of both worlds --- we propose an innovated framework for learning ACOPF, where the input load is mapped to the ACOPF solution through a neural network in a computationally efficient and reliable manner. Key to our innovation is a specific-purpose ``activation function'' defined implicitly by a QCQP and a novel loss, which enforce constraint satisfaction. We show through numerical simulations that our proposed method achieves superior feasibility rate and generation cost in situations where the existing learning-based approaches fail.}
}

@InProceedings{brumali191,
    title = {A Deep Learning Approach for Distributed Aggregative Optimization with Users' Feedback},
    author = {Brumali, Riccardo and Carnevale, Guido and Notarstefano, Giuseppe},
    pages = {1552--1564},
    abstract = {We propose a novel distributed data-driven scheme for online aggregative optimization, i.e., the framework in which agents in a network aim to cooperatively minimize the sum of local timevarying costs each depending on a local decision variable and an aggregation of all of them. We consider a ''personalized'' setup in which each cost exhibits a term capturing the user's dissatisfaction and, thus, is unknown. We enhance an existing distributed optimization scheme by endowing it with a learning mechanism based on neural networks that estimate the missing part of the gradient via users' feedback about the cost. Our algorithm combines two loops with different timescales devoted to performing optimization and learning steps. In turn, the proposed scheme also embeds a distributed consensus mechanism aimed at locally reconstructing the unavailable global information due to the presence of the aggregative variable. We prove an upper bound for the dynamic regret related to (i) the initial conditions, (ii) the temporal variations of the functions, and (iii) the learning errors about the unknown cost. Finally, we test our method via numerical simulations.}
}

@InProceedings{strong193,
    title = {A framework for evaluating human driver models using neuroimaging},
    author = {Strong, Christopher and Stocking, Kaylene and Li, Jingqi and Zhang, Tianjiao and Gallant, Jack and Tomlin, Claire},
    pages = {1565--1578},
    abstract = {Driving is a complex task which requires synthesizing multiple senses, safely reasoning about the behavior of others, and adapting to a constantly changing environment. Failures of human driving models can become failures of vehicle safety features or autonomous driving systems that rely on their predictions. Although there has been a variety of work to model human drivers, it can be challenging to determine to what extent they truly resemble the humans they attempt to mimic. The development of improved human driver models can serve as a step towards better vehicle safety. In order to better compare and develop driver models, we propose going beyond driving behavior to examine how well these models reflect the cognitive activity of human drivers. In particular, we compare features extracted from human driver models with brain activity as measured by functional magnetic resonance imaging. We demonstrate this approach on three human driver models with brain activity data from two human subjects. We find that model predictive control is a better fit for driver brain activity than classic non-predictive models, which is in good agreement with previous works that obtain better predictions of human driving behavior using model predictive control.}
}

@InProceedings{lawrence195,
    title = {Deep {H}ankel matrices with random elements},
    author = {Lawrence, Nathan and Loewen, Philip and Wang, Shuyuan and Forbes, Michael and Gopaluni, Bhushan},
    pages = {1579--1591},
    abstract = {Willems' fundamental lemma enables a trajectory-based characterization of linear systems through data-based Hankel matrices. However, in the presence of measurement noise, we ask: Is this noisy Hankel-based model expressive enough to re-identify itself? In other words, we study the output prediction accuracy from recursively applying the same persistently exciting input sequence to the model. We find an asymptotic connection to this self-consistency question in terms of the amount of data. More importantly, we also connect this question to the depth (number of rows) of the Hankel model, showing the simple act of reconfiguring a finite dataset significantly improves accuracy. We apply these insights to find a parsimonious depth for LQR problems over the trajectory space.}
}

@InProceedings{hsu196,
    title = {Robust Exploration with Adversary via {L}angevin {M}onte {C}arlo},
    author = {Hsu, Hao-Lun and Pajic, Miroslav},
    pages = {1592--1605},
    abstract = {In the realm of Deep Q-Networks (DQNs), numerous exploration strategies have demonstrated efficacy within controlled environments. However, these methods encounter formidable challenges when confronted with the unpredictability of real-world scenarios marked by disturbances. The optimization of exploration efficiency under such disturbances is not fully investigated. In response to these challenges, this work introduces a versatile reinforcement learning (RL) framework that systematically addresses the intricate interplay between exploration and robustness in dynamic and unpredictable environments. We propose a robust RL methodology, framed within a two-player max-min adversarial paradigm. This formulation is cast as a Probabilistic Action Robust Markov Decision Process (MDP), grounded in a cyber-physical perspective. Our methodology capitalizes on Langevin Monte Carlo (LMC) for Q-function exploration, facilitating iterative updates that empower both the protagonist and adversary to efficaciously explore. Notably, we extend this adversarial training paradigm to encompass robustness against delayed feedback episodes. Empirical evaluation, conducted on benchmark problems such as N-Chain and deep brain stimulation, underlines the consistent superiority of our method over baseline approaches across diverse perturbation scenarios and instances of delayed feedback.}
}

@InProceedings{chen197,
    title = {Generalized Constraint for Probabilistic Safe Reinforcement Learning},
    author = {Chen, Weiqin and Paternain, Santiago},
    pages = {1606--1618},
    abstract = {In this paper, we consider the problem of learning policies for probabilistic safe reinforcement learning (PSRL). Specifically, a safe policy or controller is one that, with high probability, maintains the trajectory of the agent in a given safe set. While the explicit gradient of the probabilistic constraint for solving PSRL directly exists, the high variance in the estimate of the gradient hinders its performance in problems with long-horizons. An alternative that is frequently explored in the literature is to consider a cumulative safe reinforcement learning (CSRL) setting. In this setting, the estimates of the constraint's gradient have less variance but are biased (worse solutions than the PSRL) and they provide an approximate solution since they solve a relaxation of the PSRL formulation. In this work, we propose a safe reinforcement learning framework with a generalized constraint for solving the PSRL problems, which we term Generalized Safe Reinforcement Learning (GSRL). Our theoretical contributions substantiate that the proposed GSRL can recover both the PSRL and CSRL settings. In addition, it can be naturally combined with any state-of-the-art safe RL algorithms like PPO-Lagrangian, TD3-Lagrangian, CPO, PCPO, etc. We evaluate the GSRL by a series of empirical experiments in the well-known safe RL benchmark Bullet-Safety-Gym, which exhibit a better return-safety trade-off than both the PSRL and CSRL formulations.}
}

@InProceedings{brunzema200,
    title = {Neural Processes with Event Triggers for Fast Adaptation to Changes},
    author = {Brunzema, Paul and Kruse, Paul and Trimpe, Sebastian},
    pages = {1619--1632},
    abstract = {Traditionally, first-principle models are used to monitor and control dynamical systems. However, modeling complex systems using first principles can be challenging. Learning the dynamics from data using neural networks has emerged as a viable alternative. In practice, some parameters of a system may vary across different system instances, but training separate neural networks for all possible parameter combinations can be infeasible. Therefore, meta-learning using, e.g., conditional neural processes (CNPs), aims to learn a prior model over the system dynamics for various parameters. These models can then adapt on deployment to the parameters of a system instance using a context set composed of past observations. However, changes in parameters can also occur online during operation and naively adding past observations across parameter variations to the context set can distort the model's latent representation, leading to inaccurate predictions over time. This paper introduces an adaptation scheme to enable CNPs to cope with such online variations. We combine a sliding window to accommodate gradual variations with the use of event triggers to detect sudden changes. The event triggers are based on concentration inequalities, they reset the context set of the CNP once observations deviate significantly from the CNP's predictions. We validate our concepts on two nonlinear dynamical systems under parameter variations and demonstrate that our approaches decrease the prediction error over time as well as their efficacy for control.}
}

@InProceedings{gracia201,
    title = {Data-Driven Strategy Synthesis for Stochastic Systems with Unknown Nonlinear Disturbances},
    author = {Gracia, Ibon and Boskos, Dimitris and Laurenti, Luca and Lahijanian, Morteza},
    pages = {1633--1645},
    abstract = {In this paper, we introduce a data-driven framework for synthesis of provably-correct controllers for general nonlinear switched systems under complex specifications. The focus is on systems with unknown disturbances whose effects on the dynamics of the system is nonlinear. The specification is assumed to be given as linear temporal logic over finite traces (LTLf) formulas. Starting from observations of either the disturbance or the state of the system, we first learn an ambiguity set that contains the unknown distribution of the disturbances with a user-defined confidence. Next, we obtain a robust Markov decision process (RMDP) as a finite abstraction of the system. By composing the RMDP with the automaton obtained from the LTLf formula and performing optimal robust value iteration on the composed RMDP, we synthesize a strategy that yields a high probability that the uncertain system satisfies the specifications. Our empirical evaluations on systems with a wide variety of disturbances show that the strategies synthesized with our approach lead to high satisfaction probabilities and validate the theoretical guarantees.}
}

@InProceedings{seyde203,
    title = {Growing {Q}-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution},
    author = {Seyde, Tim and Werner, Peter and Schwarting, Wilko and Wulfmeier, Markus and Rus, Daniela},
    pages = {1646--1661},
    abstract = {Recent reinforcement learning approaches have shown surprisingly strong capabilities of bang-bang policies for solving continuous control benchmarks. The underlying coarse action space discretizations often yield favorable exploration characteristics, while final performance does not visibly suffer in the absence of action penalization in line with optimal control theory. In robotics applications, smooth control signals are commonly preferred to reduce system wear and improve energy efficiency, while regularization via action costs can be detrimental to exploration. Our work aims to bridge this performance gap by growing discrete action spaces from coarse to fine control resolution. We take advantage of recent results in decoupled Q-learning to scale our approach to high-dimensional action spaces up to dim(A) = 38. Our work indicates that an adaptive control resolution in combination with value decomposition yields simple critic-only algorithms that enable surprisingly strong performance on continuous control tasks.}
}

@InProceedings{allen-blanchette204,
    title = {{H}amiltonian {GAN}},
    author = {Allen-Blanchette, Christine},
    pages = {1662--1674},
    abstract = {A growing body of work leverages the Hamiltonian formalism as an inductive bias for physically plausible neural network based video generation. The structure of the Hamiltonian ensures conservation of a learned quantity (e.g., energy) and imposes a phase-space interpretation on the low-dimensional manifold underlying the input video. While this interpretation has the potential to facilitate the integration of learned representations in downstream tasks, existing methods are limited in their applicability as they require a structural prior for the configuration space at design time. In this work, we present a GAN-based video generation pipeline with a learned configuration space map and Hamiltonian neural network motion model, to learn a representation of the configuration space from data. We train our model with a physics-inspired cyclic-coordinate loss function which encourages a minimal representation of the configuration space and improves interpretability. We demonstrate the efficacy and advantages of our approach on the Hamiltonian Dynamics Suite Toy Physics dataset.}
}

@InProceedings{vaskov208,
    title = {Do No Harm: A Counterfactual Approach to Safe Reinforcement Learning},
    author = {Vaskov, Sean and Schwarting, Wilko and Baker, Chris},
    pages = {1675--1687},
    abstract = {Reinforcement Learning (RL) for control has become increasingly popular due to its ability to learn feedback policies that can take into account complex representations of the environment and uncertainty. When considering safety constraints, constrained optimization approaches where agents are penalized for constraint violations are commonly used. In such methods, if agents are initialized in or must visit states where constraint violation might be inevitable, it is unclear if or how much they should be penalized. We address this challenge by formulating a constraint on the counterfactual harm of the learned policy compared to an alternate, safe policy. In a philosophical sense this method only penalizes the learner for constraint violations that it caused; in a practical sense it maintains feasibility of the optimal control problem when constraint violation is inevitable. We present simulation studies on a rover with uncertain road friction and a tractor-trailer parking environment that demonstrate our constraint formulation enables agents to learn safer policies than traditional constrained RL methods.}
}

@InProceedings{kargin212,
    title = {{W}asserstein Distributionally Robust Regret-Optimal Control over Infinite-Horizon},
    author = {Kargin, Taylan and Hajar, Joudi and Malik, Vikrant and Hassibi, Babak},
    pages = {1688--1701},
    abstract = {We investigate the Distributionally Robust Regret-Optimal (DR-RO) control of discrete-time linear dynamical systems with quadratic cost over an infinite horizon. Regret is the difference in cost obtained by a causal controller and a clairvoyant controller with access to future disturbances. We focus on the infinite-horizon framework, which results in stability guarantees. In this DR setting, the probability distribution of the disturbances resides within a Wasserstein-2 ambiguity set centered at a specified nominal distribution. Our objective is to identify a control policy that minimizes the worst-case expected regret over an infinite horizon, considering all potential disturbance distributions within the ambiguity set. In contrast to prior works, which assume time-independent disturbances, we relax this constraint to allow for time-correlated disturbances, thus actual distributional robustness. While we show that the resulting optimal controller is non-rational and lacks a finite-dimensional state-space realization, we demonstrate that it can still be uniquely characterized by a finite dimensional parameter. Exploiting this fact, we introduce an efficient numerical method to compute the controller in the frequency domain using fixed-point iterations. This method circumvents the computational bottleneck associated with the finite-horizon problem, where the semi-definite programming (SDP) solution dimension scales with the time horizon. Numerical experiments demonstrate the effectiveness and performance of our framework.}
}

@InProceedings{pantazis213,
    title = {Probably approximately correct stability of allocations in uncertain coalitional games with private sampling},
    author = {Pantazis, George and Fele, Filiberto and Fabiani, Filippo and Grammatico, Sergio and Margellos, Kostas},
    pages = {1702--1714},
    abstract = {We study coalitional games with exogenous uncertainty in the coalition value, in which each agent is allowed to have private samples of the uncertainty. As a consequence, the agents may have a different perception of stability of the grand coalition. In this context, we propose a novel methodology to study the out-of-sample coalitional rationality of allocations in the set of stable allocations (i.e., the core). Our analysis builds on the framework of probably approximately correct learning. Initially, we state a priori and a posteriori guarantees for the entire core. Furthermore, we provide a distributed algorithm to compute a compression set that determines the generalization properties of the a posteriori statements. We then refine our probabilistic robustness bounds by specialising the analysis to a single payoff allocation, taking, also in this case, both a priori and a posteriori approaches. Finally, we consider a relaxed zeta-core to include nearby allocations and also address the case of empty core. For this case, probabilistic statements are given on the eventual stability of allocations in the zeta-core.}
}

@InProceedings{naish218,
    title = {Reinforcement Learning-Driven Parametric Curve Fitting for Snake Robot Gait Design},
    author = {Naish, Jack and Rodriguez, Jacob and Zhang, Jenny and Jones, Bryson and Daddi, Guglielmo and Orekhov, Andrew and Royce, Rob and Paton, Michael and Choset, Howie and Ono, Masahiro and Thakker, Rohan},
    pages = {1715--1726},
    abstract = {Snake-inspired robots demonstrate exceptional versatility through challenging terrains such as sand, rubble, and ice. However, their high-dimensional continuous action spaces make analytical gait design challenging. Early works by Hirose (1994) showed that gait parameterization over low-dimensional spatially and temporally varying sine waves can serve as basis functions for the shape-space or central pattern generators (CPGs). Recent approaches to designing CPGs have combined annealed chain-fitting, which solves for joint angles that fit a snake robot to a desired backbone curve, and keyframe extraction, which then fits analytic shape functions to the resulting optimized joint angles. However, the non-convex optimization associated with these methods is fraught with local optima exacerbated by constraints such as actuator limits. Reinforcement Learning has emerged as a promising alternative for searching over such spaces. However, end-to-end RL approaches trained purely in simulation are vulnerable to reality distribution shifts, lack safety guarantees, and don't yield an intuitive representation of the learned gait. We propose a method that translates a gait found via policy search into a parametric representation of its component sinusoidal equations thus leveraging the strengths of both learning-based and classical approaches. Simulation and hardware experiments show that the proposed pipeline can generate parametric gaits where classical curve fitting-based approaches fail.}
}

@InProceedings{zhang219,
    title = {{P}ontryagin Neural Operator for Solving General-Sum Differential Games with Parametric State Constraints},
    author = {Zhang, Lei and Ghimire, Mukesh and Xu, Zhe and Zhang, Wenlong and Ren, Yi},
    pages = {1727--1739},
    abstract = {The values of two-player general-sum differential games are viscosity solutions to Hamilton-Jacobi-Isaacs (HJI) equations. Value and policy approximations for such games suffer from the curse of dimensionality (CoD). Alleviating CoD through physics-informed neural networks (PINN) encounters convergence issues when value discontinuity is present due to state constraints. On top of these challenges, it is often necessary to learn generalizable values and policies across a parametric space of games, e.g., for game parameter inference when information is incomplete. To address these challenges, we propose in this paper a Pontryagin-mode neural operator that outperforms existing state-of-the-art (SOTA) on safety performance across games with parametric state constraints. Our key contribution is the introduction of a costate loss defined on the discrepancy between forward and backward costate rollouts, which are computationally cheap. We show that the discontinuity of costate dynamics (in the presence of state constraints) effectively enables the learning of discontinuous values, without requiring manually supervised data as suggested by the current SOTA. More importantly, we show that the close relationship between costates and policies makes the former critical in learning feedback control policies with generalizable safety performance.}
}

@InProceedings{frison220,
    title = {Adaptive neural network based control approach for building energy control under changing environmental conditions},
    author = {Frison, Lilli and G\"{o}lzh\"{a}user, Simon},
    pages = {1740--1751},
    abstract = {Deep neural networks are adept at modeling complex relationships between input and output variables. When trained on diverse datasets, they can understand not just the specifics of individual objects but also the broader principles governing an entire object class. This research applies this principle to building heating control, a domain marked by significant heterogeneity and constant environmental changes, including renovations and changes in user behavior. Our approach involves training the network on a wide range of data instances, enhancing its adaptability to newly distributed data representing unseen scenarios. We find that Transformer-based LSTM architectures are particularly adept for this task as they are able to remember previous tasks' learning. We propose a simple yet effective control algorithm that separates system identification and forecasting from the optimization-based control step. This separation simplifies the control process while ensuring robust performance. In a wide range of simulation experiments, we demonstrate that our ``universally trained'' neural network control can adjust to changing conditions, thus reducing the need for more complex continual learning techniques. Our results suggest that training neural networks on varied datasets empowers the network with the ability to generalize and adapt beyond specific training instances, which demonstrates their effectiveness in dynamic and heterogeneous environments.}
}

@InProceedings{tan221,
    title = {Physics-Constrained Learning for {PDE} Systems with Uncertainty Quantified Port-{H}amiltonian Models},
    author = {Tan, Kaiyuan and Li, Peilun and Beckers, Thomas},
    pages = {1752--1763},
    abstract = {Modeling the dynamics of flexible objects has become an emerging topic in the community as these objects become more present in many applications, e.g., soft robotics. Due to the properties of flexible materials, the movements of soft objects are often highly nonlinear and, thus, complex to predict. Data-driven approaches seem promising for modeling those complex dynamics but often neglect basic physical principles, which consequently makes them untrustworthy and limits generalization. To address this problem, we propose a physics-constrained learning method that combines powerful learning tools and reliable physical models. Our method leverages the data collected from observations by sending them into a Gaussian process that is physically constrained by a distributed Port-Hamiltonian model. Based on the Bayesian nature of the Gaussian process, we not only learn the dynamics of the system, but also enable uncertainty quantification. Furthermore, the proposed approach preserves the compositional nature of Port-Hamiltonian systems.}
}

@InProceedings{gu226,
    title = {Proto-{MPC}: An Encoder-Prototype-Decoder Approach for Quadrotor Control in Challenging Winds},
    author = {Gu, Yuliang and Cheng, Sheng and Hovakimyan, Naira},
    pages = {1764--1775},
    abstract = {Quadrotors are increasingly used in the evolving field of aerial robotics for their agility and mechanical simplicity. However, inherent uncertainties, such as aerodynamic effects coupled with quadrotors' operation in dynamically changing environments, pose significant challenges for traditional, nominal model-based control designs. To address these challenges, we propose a multi-task meta-learning method called Encoder-Prototype-Decoder (EPD), which has the advantage of effectively balancing shared and distinctive representations across diverse training tasks. Subsequently, we integrate the EPD model into a model predictive control problem (Proto-MPC) to enhance the quadrotor's ability to adapt and operate across a spectrum of dynamically changing tasks with an efficient online implementation. We validate the proposed method in simulations, which demonstrates Proto-MPC's robust performance in trajectory tracking of a quadrotor being subject to static and spatially varying side winds.}
}

@InProceedings{kolev227,
    title = {Efficient Imitation Learning with Conservative World Models},
    author = {Kolev, Victor and Rafailov, Rafael and Hatch, Kyle and Wu, Jiajun and Finn, Chelsea},
    pages = {1776--1789},
    abstract = {We tackle the problem of policy learning from expert demonstrations without a reward function. A central challenge in this space is that these policies fail upon deployment due to issues of distributional shift, environment stochasticity, or compounding errors. Adversarial imitation learning alleviates this issue but requires additional on-policy training samples for stability, which presents a challenge in realistic domains due to inefficient learning and high sample complexity. One approach to this issue is to learn a world model of the environment, and use synthetic data for policy training. While successful in prior works, we argue that this is sub-optimal due to additional distribution shifts between the learned model and the real environment. Instead, we re-frame imitation learning as a fine-tuning problem, rather than a pure reinforcement learning one. Drawing theoretical connections to offline RL and fine-tuning algorithms, we argue that standard online world model algorithms are not well suited to the imitation learning problem. We derive a principled conservative optimization bound and demonstrate empirically that it leads to improved performance on two very challenging manipulation environments form high-dimensional raw pixel observations. We set a new state-of-the-art performance on the Franka Kitchen environment from images, requiring only 10 demos on no reward labels, as well as solving a complex dexterity manipulation task.}
}

@InProceedings{gornet228,
    title = {Restless Bandits with Rewards Generated by a Linear {G}aussian Dynamical System},
    author = {Gornet, Jonathan and Sinopoli, Bruno},
    pages = {1790--1801},
    abstract = {Decision-making under uncertainty is a fundamental problem encountered frequently and can be formulated as a stochastic multi-armed bandit problem. In the problem, the learner interacts with an environment by choosing an action at each round, where a round is an instance of an interaction. In response, the environment reveals a reward, which is sampled from a stochastic process, to the learner. The goal of the learner is to maximize cumulative reward. In this work, we assume that the rewards are the inner product of an action vector and a state vector generated by a linear Gaussian dynamical system. To predict the reward for each action, we propose a method that takes a linear combination of previously observed rewards for predicting each action's next reward. We show that, regardless of the sequence of previous actions chosen, the reward sampled for any previously chosen action can be used for predicting another action's future reward, i.e. the reward sampled for action 1 at round $t-1$ can be used for predicting the reward for action $2$ at round $t$. This is accomplished by designing a modified Kalman filter with a matrix representation that can be learned for reward prediction. Numerical evaluations are carried out on a set of linear Gaussian dynamical systems and are compared with 2 other well-known stochastic multi-armed bandit algorithms.}
}

